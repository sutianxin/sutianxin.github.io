<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Java应用学习（十一）-Kafka 学习笔记二 | Arno</title><meta name="keywords" content="📫消息队列"><meta name="author" content="天昕"><meta name="copyright" content="天昕"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="四、Kafka 的 Java 客户端4.1、生产者1、引入依赖 kafka-clients 的版本尽量与 Linux 上安装的版本一致  12345&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;"><meta property="og:type" content="article"><meta property="og:title" content="Java应用学习（十一）-Kafka 学习笔记二"><meta property="og:url" content="https://sutianxin.top/posts/2701940507.html"><meta property="og:site_name" content="Arno"><meta property="og:description" content="四、Kafka 的 Java 客户端4.1、生产者1、引入依赖 kafka-clients 的版本尽量与 Linux 上安装的版本一致  12345&lt;dependency&gt;    &lt;groupId&gt;org.apache.kafka&lt;&#x2F;groupId&gt;    &lt;artifactId&gt;kafka-clients&lt;&#x2F;artifactId&gt;"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012235444.jpg"><meta property="article:published_time" content="2021-10-12T15:45:23.000Z"><meta property="article:modified_time" content="2021-10-12T15:57:38.459Z"><meta property="article:author" content="天昕"><meta property="article:tag" content="📫消息队列"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012235444.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://sutianxin.top/posts/2701940507"><link rel="preconnect" href="//cdn.jsdelivr.net"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload='this.media="all"'><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload='this.media="all"'><script>const GLOBAL_CONFIG={root:"/",algolia:void 0,localSearch:{path:"search.xml",languages:{hits_empty:"找不到您查询的内容：${query}"}},translate:{defaultEncoding:2,translateDelay:0,msgToTraditionalChinese:"繁",msgToSimplifiedChinese:"簡"},noticeOutdate:void 0,highlight:{plugin:"highlighjs",highlightCopy:!0,highlightLang:!0},copy:{success:"复制成功",error:"复制错误",noSupport:"浏览器不支持"},relativeDate:{homepage:!1,post:!1},runtime:"天",date_suffix:{just:"刚刚",min:"分钟前",hour:"小时前",day:"天前",month:"个月前"},copyright:void 0,lightbox:"mediumZoom",Snackbar:{chs_to_cht:"你已切换为繁体",cht_to_chs:"你已切换为简体",day_to_night:"你已切换为深色模式",night_to_day:"你已切换为浅色模式",bgLight:"#49b1f5",bgDark:"#121212",position:"top-right"},source:{jQuery:"https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js",justifiedGallery:{js:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js",css:"https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css"},fancybox:{js:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js",css:"https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"}},isPhotoFigcaption:!1,islazyload:!0,isanchor:!1}</script><script id="config-diff">var GLOBAL_CONFIG_SITE={isPost:!0,isHome:!1,isHighlightShrink:!1,isToc:!0,postUpdate:"2021-10-12 23:57:38"}</script><noscript><style type="text/css">#nav{opacity:1}.justified-gallery img{opacity:1}#post-meta time,#recent-posts time{display:inline!important}</style></noscript><script>(e=>{e.saveToLocal={set:function(e,t,a){0!==a&&(a=864e5*a,a={value:t,expiry:(new Date).getTime()+a},localStorage.setItem(e,JSON.stringify(a)))},get:function(e){var t=localStorage.getItem(e);if(t){t=JSON.parse(t);if(!((new Date).getTime()>t.expiry))return t.value;localStorage.removeItem(e)}}},e.getScript=o=>new Promise((t,e)=>{const a=document.createElement("script");a.src=o,a.async=!0,a.onerror=e,a.onload=a.onreadystatechange=function(){var e=this.readyState;e&&"loaded"!==e&&"complete"!==e||(a.onload=a.onreadystatechange=null,t())},document.head.appendChild(a)}),e.activateDarkMode=function(){document.documentElement.setAttribute("data-theme","dark"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#0d0d0d")},e.activateLightMode=function(){document.documentElement.setAttribute("data-theme","light"),null!==document.querySelector('meta[name="theme-color"]')&&document.querySelector('meta[name="theme-color"]').setAttribute("content","#ffffff")};e=saveToLocal.get("theme");"dark"===e?activateDarkMode():"light"===e&&activateLightMode();e=saveToLocal.get("aside-status");void 0!==e&&("hide"===e?document.documentElement.classList.add("hide-aside"):document.documentElement.classList.remove("hide-aside"))})(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/zyoushuo/Blog@latest/hexo/css/loading_style_1.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/zhheo/JS-Heo@main/hidescrollbar/hidescrollbar.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/zyoushuo/Blog@latest/hexo/css/loading_style_2.css"><link rel="stylesheet" href="/css/cover.css"><link rel="stylesheet" href="/css/copyright.css"><link href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/font-awesome-animation.min.css"><link rel="stylesheet" href="/css/flipcountdown.css"><link rel="stylesheet" href="/css/year.css"><link rel="stylesheet" href="/css/Lete.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/PaddyLin-xum/wenjian@master/css/fontanimation.css"><link href="https://cdn.bootcdn.net/ajax/libs/botui/0.3.9/botui-theme-default.css" rel="stylesheet"><link rel="stylesheet" href="/css/iconfont.css"><link rel="stylesheet" href="/css/twikoo.css"><style>#article-container.post-content h1:before,h2:before,h3:before,h4:before,h5:before,h6:before{-webkit-animation:avatar_turn_around 1s linear infinite;-moz-animation:avatar_turn_around 1s linear infinite;-o-animation:avatar_turn_around 1s linear infinite;-ms-animation:avatar_turn_around 1s linear infinite;animation:avatar_turn_around 1s linear infinite}</style><link rel="stylesheet" href="/css/font.css" media="defer" onload='this.media="all"'><meta name="generator" content="Hexo 5.4.0"></head><body><a href="javascript:void(0);" onclick="preloader.endLoading()" title="点击跳过动画"><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="wizard-scene"><div class="wizard-objects"><div class="wizard-square"></div><div class="wizard-circle"></div><div class="wizard-triangle"></div></div><div class="wizard"><div class="wizard-body"></div><div class="wizard-right-arm"><div class="wizard-right-hand"></div></div><div class="wizard-left-arm"><div class="wizard-left-hand"></div></div><div class="wizard-head"><div class="wizard-beard"></div><div class="wizard-face"><div class="wizard-adds"></div></div><div class="wizard-hat"><div class="wizard-hat-of-the-hat"></div><div class="wizard-four-point-star --first"></div><div class="wizard-four-point-star --second"></div><div class="wizard-four-point-star --third"></div></div></div></div></div></div></a><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="author-avatar"><img class="avatar-img" data-lazy-src="/img/avatar.png" onerror='onerror=null,src="/img/friend_404.gif"' alt="avatar"></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">85</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">17</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">12</div></a></div></div></div><hr><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gucheng"></use></svg><span>首页</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gushu1"></use></svg><span>文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gushu"></use></svg><span>归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-guwan"></use></svg><span>标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gujianzhu-01"></use></svg><span>分类</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofengwenfangsibaoyantaimoyan_huaban_huaban"></use></svg><span>留言板</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-red_envelope"></use></svg><span>拓展</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/random/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingbaozhu"></use></svg><span>随机文章</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/adjust/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingguadeng"></use></svg><span>更换背景</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/statistics/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingjiutan"></use></svg><span>文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://sutianxin.gitee.io"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingzhongguojie"></use></svg><span>国内镜像</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingzhuzi"></use></svg><span>导航栏</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bb/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxinghulu"></use></svg><span>哔哔</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/music/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingshanzi"></use></svg><span>音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofenggudaiqiwujiuqijue_huaban_huaban"></use></svg><span>社交</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofenggudaileqixun_huaban_huaban_huaban"></use></svg><span>友链</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofenggudaileqibianzhong_huaban_huaban_huaban_huaban"></use></svg><span>朋友圈</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-guqin"></use></svg><span>关于我</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image:url(https://gitee.com/sutianxin/blogImage/raw/master/img/20211012235444.jpg)"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Arno</a></span><span id="weather-v2-plugin-simple"></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gucheng"></use></svg><span>首页</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gushu1"></use></svg><span>文章</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gushu"></use></svg><span>归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-guwan"></use></svg><span>标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gujianzhu-01"></use></svg><span>分类</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/comments/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofengwenfangsibaoyantaimoyan_huaban_huaban"></use></svg><span>留言板</span></a></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-red_envelope"></use></svg><span>拓展</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/random/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingbaozhu"></use></svg><span>随机文章</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/adjust/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingguadeng"></use></svg><span>更换背景</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/statistics/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingjiutan"></use></svg><span>文章统计</span></a></li><li><a class="site-page child faa-parent animated-hover" target="_blank" rel="noopener" href="https://sutianxin.gitee.io"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingzhongguojie"></use></svg><span>国内镜像</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/box/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingzhuzi"></use></svg><span>导航栏</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bb/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxinghulu"></use></svg><span>哔哔</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/music/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-xianxingshanzi"></use></svg><span>音乐</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="javascript:void(0);"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofenggudaiqiwujiuqijue_huaban_huaban"></use></svg><span>社交</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofenggudaileqixun_huaban_huaban_huaban"></use></svg><span>友链</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-gufengwujianzhongguofenggudaileqibianzhong_huaban_huaban_huaban_huaban"></use></svg><span>朋友圈</span></a></li></ul></div><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/about/"><svg class="icon faa-tada" aria-hidden="true"><use xlink:href="#icon-guqin"></use></svg><span>关于我</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Java应用学习（十一）-Kafka 学习笔记二</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-12T15:45:23.000Z" title="发表于 2021-10-12 23:45:23">2021-10-12</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2021-10-12T15:57:38.459Z" title="更新于 2021-10-12 23:57:38">2021-10-12</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/">框架学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">12k</span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="四、Kafka-的-Java-客户端"><a href="#四、Kafka-的-Java-客户端" class="headerlink" title="四、Kafka 的 Java 客户端"></a>四、Kafka 的 Java 客户端</h1><h2 id="4-1、生产者"><a href="#4-1、生产者" class="headerlink" title="4.1、生产者"></a>4.1、生产者</h2><h3 id="1、引入依赖"><a href="#1、引入依赖" class="headerlink" title="1、引入依赖"></a>1、引入依赖</h3><blockquote><p>kafka-clients 的版本尽量与 Linux 上安装的版本一致</p></blockquote><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.kafka&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;kafka-clients&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;kafka.version&#125;&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><h3 id="2、基本实现"><a href="#2、基本实现" class="headerlink" title="2、基本实现"></a>2、基本实现</h3><ul><li>创建一个静态常量，用于指定主题名称</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 主题名称</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br></pre></td></tr></table></figure><ul><li>定义一个 Order 类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Order</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> orderId;</span><br><span class="line">    <span class="keyword">private</span> String productName;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>创建一个 Properties 对象，在其中设置配置属性的值</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//1 创建一个 Properties 对象，设置属性值</span></span><br><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line"><span class="comment">//2 设置 Kafka Server 的 IP 地址和端口号</span></span><br><span class="line"><span class="comment">// 如果是 Kafka 集群，那么多个 IP:port 之间使用 &quot;,&quot; 分开</span></span><br><span class="line">properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;120.78.198.32:9092&quot;</span>);</span><br><span class="line"><span class="comment">//3 将待发送的 key 从字符串序列化为字节数组</span></span><br><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line"><span class="comment">//4 把发送消息 value 从字符串序列化为字节数组</span></span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br></pre></td></tr></table></figure><ul><li>创建一个消息生产者，需要传入上面定义的 Properties 对象</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//5 创建一个 Producer 对象，传入上面定义的 Properties 对象</span></span><br><span class="line">Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br></pre></td></tr></table></figure><ul><li>定义一条消息对象</li></ul><blockquote><p>在发送消息时，可以不传入 key ，发出的消息是 order 对象的字符串形式，而key 的作用是用来告诉 Kafka 往哪个分区上发送消息，具体发送分区的计算公式是：hash(key) % 分区数目</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//6 创建一个 Order 对象</span></span><br><span class="line">Order order = <span class="keyword">new</span> Order(<span class="number">1</span>, <span class="string">&quot;商品名...&quot;</span>);</span><br><span class="line"><span class="comment">//7 构造一个 ProducerRecord 对象，这个对象就是我们要发送的消息</span></span><br><span class="line"><span class="comment">// key 的作用是用来告诉 Kafka 往哪个分区上发送消息</span></span><br><span class="line"><span class="comment">// 具体发送分区的计算公式是：hash(key) % 分区数目</span></span><br><span class="line">ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(TOPIC_NAME, order.getOrderId() + <span class="string">&quot;&quot;</span>, JSON.toJSONString(order));</span><br></pre></td></tr></table></figure><ul><li>使用同步阻塞的方式发送消息</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//8 使用 Producer 的 send 方法发送数据</span></span><br><span class="line"><span class="comment">// 等待消息发送成功的同步阻塞方法</span></span><br><span class="line">RecordMetadata recordMetadata = producer.send(record).get();</span><br></pre></td></tr></table></figure><ul><li>打印结果</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">System.out.println(<span class="string">&quot;同步方式发送消息结果: topic-&quot;</span> + recordMetadata.topic() + <span class="string">&quot;|partition-&quot;</span> + recordMetadata.partition() + <span class="string">&quot;|offset-&quot;</span> + recordMetadata.offset());</span><br></pre></td></tr></table></figure><blockquote><p>全部代码如下</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 主题名称</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> ExecutionException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">//1 创建一个 Properties 对象，设置属性值</span></span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    <span class="comment">//2 设置 Kafka Server 的 IP 地址和端口号</span></span><br><span class="line">    <span class="comment">// 如果是 Kafka 集群，那么多个 IP:port 之间使用 &quot;,&quot; 分开</span></span><br><span class="line">    properties.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;120.78.198.32:9092&quot;</span>);</span><br><span class="line">    <span class="comment">//3 将待发送的 key 从字符串序列化为字节数组</span></span><br><span class="line">    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    <span class="comment">//4 把发送消息 value 从字符串序列化为字节数组</span></span><br><span class="line">    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    <span class="comment">//5 创建一个 Producer 对象，传入上面定义的 Properties 对象</span></span><br><span class="line">    Producer&lt;String, String&gt; producer = <span class="keyword">new</span> KafkaProducer&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">//6 创建一个 Order 对象</span></span><br><span class="line">    Order order = <span class="keyword">new</span> Order(<span class="number">1</span>, <span class="string">&quot;商品名...&quot;</span>);</span><br><span class="line">    <span class="comment">//7 构造一个 ProducerRecord 对象，这个对象就是我们要发送的消息</span></span><br><span class="line">    <span class="comment">// key 的作用是用来告诉 Kafka 往哪个分区上发送消息</span></span><br><span class="line">    <span class="comment">// 具体发送分区的计算公式是：hash(key) % 分区数目</span></span><br><span class="line">    ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(TOPIC_NAME, order.getOrderId() + <span class="string">&quot;&quot;</span>, JSON.toJSONString(order));</span><br><span class="line">    <span class="comment">//8 使用 Producer 的 send 方法发送数据</span></span><br><span class="line">    <span class="comment">// 等待消息发送成功的同步阻塞方法</span></span><br><span class="line">    RecordMetadata recordMetadata = producer.send(record).get();</span><br><span class="line">    System.out.println(<span class="string">&quot;同步方式发送消息结果: topic-&quot;</span> + recordMetadata.topic() + <span class="string">&quot;|partition-&quot;</span> + recordMetadata.partition() + <span class="string">&quot;|offset-&quot;</span> + recordMetadata.offset());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、发送消息到指定分区上"><a href="#3、发送消息到指定分区上" class="headerlink" title="3、发送消息到指定分区上"></a>3、发送消息到指定分区上</h3><blockquote><p>在创建 ProducerRecord 对象时，往构造方法中传入要发送的分区上</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ProducerRecord&lt;String, String&gt; record = <span class="keyword">new</span> ProducerRecord&lt;&gt;(TOPIC_NAME, <span class="number">0</span>,order.getOrderId() + <span class="string">&quot;&quot;</span>, JSON.toJSONString(order));</span><br></pre></td></tr></table></figure><h3 id="4、同步发送"><a href="#4、同步发送" class="headerlink" title="4、同步发送"></a>4、同步发送</h3><blockquote><p>⽣产者同步发消息，在收到 kafka 的 ack 告知发送成功之前⼀直处于阻塞状态</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//8 使用 Producer 的 send 方法发送数据</span></span><br><span class="line"><span class="comment">// 等待消息发送成功的同步阻塞方法</span></span><br><span class="line">RecordMetadata recordMetadata = producer.send(record).get();</span><br><span class="line">System.out.println(<span class="string">&quot;同步方式发送消息结果: topic-&quot;</span> + recordMetadata.topic() + <span class="string">&quot;|partition-&quot;</span> + recordMetadata.partition() + <span class="string">&quot;|offset-&quot;</span> + recordMetadata.offset());</span><br></pre></td></tr></table></figure><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211010222633.png" alt="image-20211010222633539"></p><h3 id="5、异步发消息"><a href="#5、异步发消息" class="headerlink" title="5、异步发消息"></a>5、异步发消息</h3><blockquote><p>⽣产者发消息，发送完后不⽤等待 broker 给回复，直接执⾏下⾯的业务逻辑。</p><p>可以提供 <code>callback</code> ，让 broker 异步的调⽤ callback，告知⽣产者，消息发送的结果</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//9 异步调用</span></span><br><span class="line">producer.send(record, <span class="keyword">new</span> Callback() &#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCompletion</span><span class="params">(RecordMetadata recordMetadata, Exception e)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (e != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;消息发送失败:&quot;</span> + e.getMessage());</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (recordMetadata != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.out.println(<span class="string">&quot;异步⽅式发送消息结果：&quot;</span>+<span class="string">&quot;topic-&quot;</span>+ recordMetadata.topic()+<span class="string">&quot;|partition-&quot;</span>+recordMetadata.partition()+<span class="string">&quot;|offset-&quot;</span>+recordMetadata.offset());            </span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure><h3 id="6、关于生产者的-ACK-参数配置"><a href="#6、关于生产者的-ACK-参数配置" class="headerlink" title="6、关于生产者的 ACK 参数配置"></a>6、关于生产者的 ACK 参数配置</h3><blockquote><p>在同步发消息的场景下：⽣产者发动 broker 上后，ack 会有3种不同的选择：</p></blockquote><ul><li><code>acks = 0</code></li></ul><blockquote><p>表示 producer 不需要等待任何 broker 确认收到消息的回复，就可以继续发送下⼀条消息。性能最⾼，但是最容易丢消息。</p></blockquote><ul><li><code>acks = 1</code>（默认）</li></ul><blockquote><p>⾄少要等待 <code>leader</code> 已经成功将数据写⼊本地 log，但是不需要等待所有 <code>follower</code> 是否成功写⼊。就可以继续发送下⼀条消息。</p><p><strong>这种情况下，如果 <code>follower</code> 没有成功备份数据，⽽此时 <code>leader</code> ⼜挂掉，则消息会丢失</strong>。</p></blockquote><ul><li><code>acks = -1</code> 或 <code>all</code></li></ul><blockquote><p>需要等待 <code>min.insync.replicas</code> (默认为1，推荐配置⼤于等于2)这个参数配置的副本个数都成功写⼊⽇志，<strong>这种策略会保证只要有⼀个备份存活就不会丢失数据</strong>。</p></blockquote><ol><li><strong>这是最强的数据保证</strong>。</li><li>⼀般除⾮是⾦融级别，或跟钱打交道的场景才会使⽤这种配置。</li></ol><ul><li>如何设置？</li></ul><blockquote><p>在 Properties 对象中指定配置的值</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ProducerConfig.ACKS_CONFIG,<span class="string">&quot;1&quot;</span>);</span><br></pre></td></tr></table></figure><h3 id="9、其他细节"><a href="#9、其他细节" class="headerlink" title="9、其他细节"></a>9、其他细节</h3><ul><li>发送会默认会重试 3 次，每次间隔 100ms</li><li><strong>发送的消息会先进⼊到本地缓冲区（32mb），kakfa 会跑⼀个线程，该线程去缓冲区中<br>取 16k 的数据，发送到kafka，如果到10毫秒数据没取满16k，也会发送⼀次</strong>。</li></ul><blockquote><p>发送消息时不是一条消息建立一次连接、发送一次，而是先存在一个缓冲区中，然后另起一个线程去发送消息（一次发送 16k ）</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211010234216.png" alt="image-20211010234216403"></p><ul><li>这两个配置可以在生产者中配置</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//配置生产者本地缓冲区大小,33554432 即 32 M</span></span><br><span class="line">properties.put(ProducerConfig.BUFFER_MEMORY_CONFIG, <span class="number">33554432</span>);</span><br><span class="line"><span class="comment">//配置生产者批量发送消息的大小，默认是 16 k，即 16384</span></span><br><span class="line">properties.put(ProducerConfig.BATCH_SIZE_CONFIG, <span class="number">16384</span>);</span><br></pre></td></tr></table></figure><h2 id="4-2、消费者"><a href="#4-2、消费者" class="headerlink" title="4.2、消费者"></a>4.2、消费者</h2><h3 id="1、基本实现"><a href="#1、基本实现" class="headerlink" title="1、基本实现"></a>1、基本实现</h3><ul><li>创建两个常量，用于指定主题名和消费组名</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 主题名称</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者组名</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONSUMER_GROUP_NAME = <span class="string">&quot;testGroup&quot;</span>;</span><br></pre></td></tr></table></figure><ul><li>创建一个 Properties 对象，设置配置值</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;&quot;</span>);</span><br><span class="line">properties.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_NAME);</span><br><span class="line"><span class="comment">// 配置键和值的序列号器</span></span><br><span class="line">properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br></pre></td></tr></table></figure><ul><li>创建一个 KafkaConsumer 消费者对象，传入上面定义的 Properties 对象</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建一个消费者对象，传入 Properties 对象</span></span><br><span class="line">KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br></pre></td></tr></table></figure><ul><li>令消费者对象订阅主题列表</li></ul><blockquote><p>一个消费者其实可以消费多个主题</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 消费者对象订阅主题列表</span></span><br><span class="line">consumer.subscribe(Collections.singletonList(TOPIC_NAME));</span><br></pre></td></tr></table></figure><ul><li>进行消息拉取</li></ul><blockquote><p>使用消费者的 poll() 方法对消息进行长轮询</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * poll() API 是拉取消息的长轮询</span></span><br><span class="line"><span class="comment">     * 在 1000 ms 内拉取过来的消息都会放在 ConsumerRecords 列表中</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">        System.out.printf(<span class="string">&quot;收到消息：partition = %d,offset = %d, key = %s, value = %s%n&quot;</span>, record.partition(),</span><br><span class="line">        record.offset(), record.key(), record.value());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>所有代码</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 主题名称</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 消费者组名</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String CONSUMER_GROUP_NAME = <span class="string">&quot;testGroup&quot;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">    Properties properties = <span class="keyword">new</span> Properties();</span><br><span class="line">    properties.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;&quot;</span>);</span><br><span class="line">    properties.put(ConsumerConfig.GROUP_ID_CONFIG, CONSUMER_GROUP_NAME);</span><br><span class="line">    <span class="comment">// 配置键和值的序列号器</span></span><br><span class="line">    properties.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    properties.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());</span><br><span class="line">    <span class="comment">// 创建一个消费者对象，传入 Properties 对象</span></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(properties);</span><br><span class="line">    <span class="comment">// 消费者对象订阅主题列表</span></span><br><span class="line">    consumer.subscribe(Collections.singletonList(TOPIC_NAME));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * poll() API 是拉取消息的长轮询</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.printf(<span class="string">&quot;收到消息：partition = %d,offset = %d, key = %s, value = %s%n&quot;</span>, record.partition(),</span><br><span class="line">            record.offset(), record.key(), record.value());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2、Kafka-消费者-offset-的自动提交和手动提交"><a href="#2、Kafka-消费者-offset-的自动提交和手动提交" class="headerlink" title="2、Kafka 消费者 offset 的自动提交和手动提交"></a>2、Kafka 消费者 offset 的自动提交和手动提交</h3><blockquote><p>在消费者对消息进行消费时，需要向 Kafka 提交 offset ，有自动提交和手动提交两种模式</p><p>无论是自动提交还是手动提交，都需要把所属的消费组 + 消费的某个主题 + 消费的某个分区及消费的偏移量，这样的信息提交到集群的 <code>_comsumer_offsets</code> 主题中</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011225954.png" alt="image-20211011103251825"></p><ul><li>offset 的自动提交（默认）</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 是否⾃动提交offset，默认就是true</span></span><br><span class="line">props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;true&quot;</span>);</span><br><span class="line"><span class="comment">// ⾃动提交offset的间隔时间</span></span><br><span class="line">props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, <span class="string">&quot;1000&quot;</span>);</span><br></pre></td></tr></table></figure><blockquote><p>消费者 poll 到消息后，会自动向 Broker 的 <code>_comsumer_offsets</code> 主题提交当前主题 - 分区消费的偏移量。</p><p><strong>自动提交可能导致消息丢失</strong>，因为消费者可能一次性 poll 下来多条消息，消费者还没消费 poll 下来的消息就自动提交了偏移量，<strong>如果消费者在没有完全消费 poll 下来的消息前就挂了，那么那一部分已经 poll 下来但还没有消费完的消息就会丢失</strong>，下一个消费者会从已提交的 offset 的下一个位置开始消费消息。</p></blockquote><ul><li>offset 的手动提交</li></ul><blockquote><p>在消费消息时 / 后再提交 offset</p></blockquote><ol><li>设置手动提交参数</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, <span class="string">&quot;false&quot;</span>);</span><br></pre></td></tr></table></figure><ol start="2"><li>在消费完消息后进行手动提交（手动提交 offset 又分为手动同步提交和手动异步提交）</li></ol><blockquote><p>手动同步提交</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (records.count &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 手动同步提交 offset ，当前线程会阻塞直到 offset 提交成功</span></span><br><span class="line">    <span class="comment">// 一般使用同步提交，因为提交之后一般也没有什么逻辑代码了</span></span><br><span class="line">    consumer.commitSync();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>手动异步提交，定义一个回调方法， Kafka 集群会调用这个方法通知手动提交是否成功</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (records.count() &gt; <span class="number">0</span>) &#123;</span><br><span class="line"><span class="comment">// ⼿动异步提交offset，当前线程提交offset不会阻塞，可以继续处理后⾯的程序逻辑</span></span><br><span class="line">    consumer.commitAsync((offsets, exception) -&gt; &#123;</span><br><span class="line">        <span class="keyword">if</span> (exception != <span class="keyword">null</span>) &#123;</span><br><span class="line">            System.err.println(<span class="string">&quot;Commit failed for &quot;</span> + offsets);</span><br><span class="line">            System.err.println(<span class="string">&quot;Commit failed exception: &quot;</span> + exception.getStackTrace());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="3、消费者-poll-消息的过程"><a href="#3、消费者-poll-消息的过程" class="headerlink" title="3、消费者 poll 消息的过程"></a>3、消费者 poll 消息的过程</h3><ul><li>消费者建立与 Broker 之间的联系，开始 poll 消息</li><li>默认一次性 poll 500条消息</li></ul><blockquote><p>可以根据消费速度的快慢来设置，<strong>因为如果两次 poll 的时间如果超出了 30s 的时间间隔，kafka 会认为其消费能⼒过弱，将其踢出消费组。将分区分配给其他消费者</strong>。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, <span class="number">500</span>);</span><br></pre></td></tr></table></figure><ul><li>设置消费者两次 poll 之间的时间间隔</li></ul><blockquote><p>消费者在被剔除消费组后，会触发 rebalance 机制， rebalance 机制会造成性能开销</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.MAX_POLL_INTERVAL_MS_CONFIG, <span class="number">30</span> * <span class="number">1000</span>);</span><br></pre></td></tr></table></figure><ul><li>代码中配置了长轮询 poll 的时间，如果每隔 1s 内没有 poll 到任何消息，则继续去 poll 消息，循环往复，直到 poll 到消息。<strong>如果超出了 1s，则此次⻓轮询结束</strong>。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ConsumerRecords&lt;String, String&gt; records = consumer.poll(Duration.ofMillis(<span class="number">1000</span>));</span><br></pre></td></tr></table></figure><ol><li>如果一次 poll 到的消息的条数为配置中 <code>ConsumerConfig.MAX_POLL_RECORDS_CONFIG</code> 配置的条数，那么直接执行下面的逻辑</li><li>如果一次 poll 到的消息没有达到配置中 <code>ConsumerConfig.MAX_POLL_RECORDS_CONFIG</code> 配置的条数，且时间在 poll 方法配置的时间 <code>Duration.ofMillis(1000)</code> 内，那么长轮询会继续 poll ，直到 poll 到的消息条数达到配置的条数或者时间达到配置时间。</li><li>如果多次 poll 都没有达到配置中 <code>ConsumerConfig.MAX_POLL_RECORDS_CONFIG</code> 配置的条数，且时间已经超过 poll 方法配置的时间 <code>Duration.ofMillis(1000)</code> ，那么直接执行下面的逻辑</li></ol><ul><li>消费者给 Kafka 发送心跳的时间间隔</li></ul><blockquote><p>Kafka 使用 Zookeeper 来维持心跳</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.HEARTBEAT_INTERVAL_MS_CONFIG, <span class="number">1000</span>);</span><br></pre></td></tr></table></figure><ul><li>Kafka 如果超过 10 秒没有收到消费者的⼼跳，则会把消费者踢出消费组，进⾏ rebalance，把分区分配给其他消费者。</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.SESSION_TIMEOUT_MS_CONFIG, <span class="number">10</span> * <span class="number">1000</span>);</span><br></pre></td></tr></table></figure><h3 id="4、消费者指定主题的分区进行消费"><a href="#4、消费者指定主题的分区进行消费" class="headerlink" title="4、消费者指定主题的分区进行消费"></a>4、消费者指定主题的分区进行消费</h3><ul><li>可以通过消费者对象的 assign 方法来指定分区</li></ul><blockquote><p>下面的代码中，为当前消费者指定消费 <code>my-replicated-topic</code> 主题中的第 0 个分区。</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">consumer.assign(Collections.singletonList(<span class="keyword">new</span> TopicPartition(TOPIC_NAME, <span class="number">0</span>)));</span><br></pre></td></tr></table></figure><h3 id="5、消息的回溯消费"><a href="#5、消息的回溯消费" class="headerlink" title="5、消息的回溯消费"></a>5、消息的回溯消费</h3><blockquote><p>Kafka 支持两种回溯方式回溯消息，一种是基于消息偏移量 offset 回溯，一种是基于时间点回溯，还有一种方式是从头消费</p></blockquote><ul><li>从头消费</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定消费该主题的第一个分区</span></span><br><span class="line">consumer.assign(Collections.singletonList(<span class="keyword">new</span> TopicPartition(TOPIC_NAME, <span class="number">0</span>)));</span><br><span class="line"><span class="comment">// 从头开始消费</span></span><br><span class="line">consumer.seekToBeginning(Collections.singletonList(<span class="keyword">new</span> TopicPartition(TOPIC_NAME, <span class="number">0</span>)));</span><br></pre></td></tr></table></figure><ul><li>基于消息偏移量（offset）回溯</li></ul><blockquote><p>在 Kafka 的每个分区中，每条消息都有唯一的一个 offset 值，这个值用于表示这条消息在该分区中的位置，在消费者消费完这条消息后，会将这条消息的 offset 提交到 Broker ，下条消息从这个位置后开始消费。</p><p><strong>基于消息偏移量回溯很简单，只需要重置 offset ，然后消费者会从该 offset 之后开始消费</strong></p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 指定消费该主题的第一个分区</span></span><br><span class="line">consumer.assign(Collections.singletonList(<span class="keyword">new</span> TopicPartition(TOPIC_NAME, <span class="number">0</span>)));</span><br><span class="line"><span class="comment">// 指定 offset 消费</span></span><br><span class="line">consumer.seek(Collections.singletonList(<span class="keyword">new</span> TopicPartition(TOPIC_NAME, <span class="number">0</span>)),<span class="number">10</span>);</span><br></pre></td></tr></table></figure><ul><li>基于时间点回溯</li></ul><blockquote><p>我们先了解一下 Kafka 存储消息的文件格式，Kafka 存储消息是以日志的形式存储的，每个分区对应一个日志，但日志不是一个文件，而是由多个文件组成的。</p><p>日志文件都存储在一个文件夹中，文件夹格式为 topic-0。</p></blockquote><ol><li>topic 为 kafka 对应的主题名称，0 是分区所在的分区号。</li><li>文件夹中存储着 4 个日志，分别为<strong>日志分段文件</strong>、<strong>偏移量索引文件</strong>、<strong>时间戳索引文件</strong> 和 <strong>其他文件</strong></li></ol><blockquote><p>那么是怎么通过时间戳回溯的呢？</p></blockquote><ol><li><p>将要回溯的时间换算为时间戳 <code>timestamp</code> （yyyy-MM-dd HH:mm:ss –&gt; long）</p></li><li><p>根据时间戳 <code>timestamp</code> 在时间戳索引文件中找到不大于这个时间戳 <code>timestamp</code> 的最大偏移量</p></li><li><p>找到偏移量后，找到对应消息的位置 position （查找过程后面补充）</p></li><li><p>通过 position 定位消息，获取该消息的生成时间，将这个时间与 <code>timestamp</code> 进行对比，然后按顺序逐渐和后面的消息进行比对，如果前一个消息的生成时间小于 <code>timestamp</code> 且后一个消息的生成时间大于 <code>timestamp</code> ，那么当前这个位置就是要回溯的点，获取消息的 offset ，对消费者消费记录的 offset 进行重置，回溯完成</p></li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">   List&lt;PartitionInfo&gt; topicPartitions = consumer.partitionsFor(TOPIC_NAME);</span><br><span class="line"><span class="comment">//从1⼩时前开始消费</span></span><br><span class="line">   <span class="keyword">long</span> fetchDataTime = <span class="keyword">new</span> Date().getTime() - <span class="number">1000</span> * <span class="number">60</span> * <span class="number">60</span>;</span><br><span class="line">   Map&lt;TopicPartition, Long&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">   <span class="keyword">for</span> (PartitionInfo par : topicPartitions) &#123;</span><br><span class="line">       map.put(<span class="keyword">new</span> TopicPartition(TOPIC_NAME, par.partition()), fetchDataTime);</span><br><span class="line">   &#125;</span><br><span class="line">   Map&lt;TopicPartition, OffsetAndTimestamp&gt; parMap = consumer.offsetsForTimes(map);</span><br><span class="line">   <span class="keyword">for</span> (Map.Entry&lt;TopicPartition, OffsetAndTimestamp&gt; entry : parMap.entrySet()) &#123;</span><br><span class="line">       TopicPartition key = entry.getKey();</span><br><span class="line">       OffsetAndTimestamp value = entry.getValue();</span><br><span class="line">       <span class="keyword">if</span> (key == <span class="keyword">null</span> || value == <span class="keyword">null</span>) &#123;</span><br><span class="line">           <span class="keyword">continue</span>;</span><br><span class="line">       &#125;</span><br><span class="line">       <span class="keyword">long</span> offset = value.offset();</span><br><span class="line">       System.out.println(<span class="string">&quot;partition-&quot;</span> + key.partition() + <span class="string">&quot;|offset-&quot;</span> + offset);</span><br><span class="line">       System.out.println();</span><br><span class="line">       <span class="comment">//根据消费⾥的timestamp确定offset</span></span><br><span class="line">       <span class="keyword">if</span> (value != <span class="keyword">null</span>) &#123;</span><br><span class="line">           consumer.assign(Collections.singletonList(key));</span><br><span class="line">           consumer.seek(key, offset); 	</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><h3 id="6、指定新消费组的消费偏移量"><a href="#6、指定新消费组的消费偏移量" class="headerlink" title="6、指定新消费组的消费偏移量"></a>6、指定新消费组的消费偏移量</h3><blockquote><p>当消费主题的是⼀个新的消费组，或者指定 offset 的消费⽅式，offset 不存在，那么应该如何消费？</p></blockquote><ul><li><code>latest</code>（默认）</li></ul><blockquote><p>只消费自己启动后发送到主题的消息</p></blockquote><ul><li><code>earliest</code></li></ul><blockquote><p>第一次从头开始消费，以后按照消费 offset 的记录继续消费，这个需要区别于 <code>consumer.seekToBeginning</code> （每次都从头开始消费）</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, <span class="string">&quot;earliest&quot;</span>);</span><br></pre></td></tr></table></figure><h1 id="五、Spring-Boot-使用-Kafka"><a href="#五、Spring-Boot-使用-Kafka" class="headerlink" title="五、Spring Boot 使用 Kafka"></a>五、Spring Boot 使用 Kafka</h1><h2 id="5-1、前期准备"><a href="#5-1、前期准备" class="headerlink" title="5.1、前期准备"></a>5.1、前期准备</h2><ul><li>创建一个 Spring Boot 项目，引入相关依赖</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">parent</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-parent<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.3.RELEASE<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">relativePath</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">parent</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.alibaba<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>fastjson<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.78<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-web<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-test<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">scope</span>&gt;</span>test<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-kafka<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>创建一个实体类</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Data</span></span><br><span class="line"><span class="meta">@NoArgsConstructor</span></span><br><span class="line"><span class="meta">@AllArgsConstructor</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Product</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String productId;</span><br><span class="line">    <span class="keyword">private</span> String productName;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-2、创建配置文件"><a href="#5-2、创建配置文件" class="headerlink" title="5.2、创建配置文件"></a>5.2、创建配置文件</h2><blockquote><p>一般来说，一个应用最多只扮演生产者 / 消费者之中的一个角色，这个时候只需要添加对应角色的配置即可。</p></blockquote><h3 id="1、生产者相关配置"><a href="#1、生产者相关配置" class="headerlink" title="1、生产者相关配置"></a>1、生产者相关配置</h3><ul><li>重试次数（retries）</li></ul><blockquote><p>设置一个大于 0 的值，默认为 3 ，即生产者在发送失败时会默认重发三次</p></blockquote><ul><li>批量发给 Kafka 的数据大小（batch-size）</li><li>生产者本地缓冲区（buffer-memory）</li></ul><blockquote><p>这两个配置联系十分紧密，生产者不是每接收到一条消息就直接发给 Kafka ，而是先将消息存储在本地的一个缓冲区中，这个缓冲区的大小为 <code>buffer-memory</code> ，同时在后台另起一个线程，让这个线程每次从这个缓冲区中拿出 <code>batch-size</code> 大小的数据发送给 Kafka 。</p></blockquote><ul><li>生产者发送消息的确认模式（acks）</li></ul><blockquote><p>默认为 1 ，⾄少要等待 leader 已经成功将数据写⼊本地 log，但是不需要等待所有 follower 是否成功写⼊，就可以继续发送下⼀条消息。</p><p>效率与可靠性折中的一种选择</p></blockquote><ul><li>键和消息记录的编解码方式（key-serializer / value-serializer）</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="comment"># 生产者</span></span><br><span class="line">    <span class="attr">producer:</span></span><br><span class="line">      <span class="comment"># 设置一个大于 0 的值，则客户端会将发送失败的记录重新发送</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">      <span class="comment"># 生产者一次从本地缓冲区中取出 batch-size 大小的数据发送给 Kafka</span></span><br><span class="line">      <span class="attr">batch-size:</span> <span class="number">16384</span></span><br><span class="line">      <span class="comment"># 设置 acks 为 1， ⾄少要等待 leader 已经成功将数据写⼊本地 log，但是不需要等待所有 follower 是否成功写⼊。就可以继续发送下⼀条消息。</span></span><br><span class="line">      <span class="attr">acks:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment"># 指定消息 key 与消息体的编解码方式</span></span><br><span class="line">      <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="comment"># 生产者本地缓冲区，默认为 32m</span></span><br><span class="line">      <span class="attr">buffer-memory:</span> <span class="number">33554432</span></span><br></pre></td></tr></table></figure><h3 id="2、消费者相关配置"><a href="#2、消费者相关配置" class="headerlink" title="2、消费者相关配置"></a>2、消费者相关配置</h3><ul><li><p>消费者的消费组 id （group-id）</p></li><li><p>是否启用自动提交（enable-auto-commit）</p></li><li><p>新消费组的消费偏移量选取策略（auto-offset-reset）</p></li><li><p>一次长轮询中从 Broker 中拉取的消息条数的最大值（max-poll-records）</p></li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="attr">consumer:</span></span><br><span class="line">      <span class="comment"># 指定消费组 id</span></span><br><span class="line">      <span class="attr">group-id:</span> <span class="string">default-group</span></span><br><span class="line">      <span class="comment"># 关闭自动提交</span></span><br><span class="line">      <span class="attr">enable-auto-commit:</span> <span class="literal">false</span></span><br><span class="line">      <span class="comment"># 当出现一个新的消费组或者需要传入 offset 但 offset 为空时，指定其从头开始消费</span></span><br><span class="line">      <span class="comment"># 但不是每次都是从头开始消费</span></span><br><span class="line">      <span class="attr">auto-offset-reset:</span> <span class="string">earliest</span></span><br><span class="line">      <span class="attr">key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="attr">value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="comment"># 消费者一次从 Broker 最多拉取 500 条消息</span></span><br><span class="line">      <span class="attr">max-poll-records:</span> <span class="number">500</span></span><br></pre></td></tr></table></figure><h3 id="3、额外配置"><a href="#3、额外配置" class="headerlink" title="3、额外配置"></a>3、额外配置</h3><ul><li>Kafka-Server 的地址（bootstrap-servers）</li></ul><blockquote><p>如果是 Kafka 集群，那么在多个 IP:port 之间使用 <code>,</code> 隔开</p></blockquote><ul><li>Kafka 消费者的消费模式</li></ul><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="comment"># kafka-server 的地址，如果是集群，那么 ip:port 之间使用 &quot;,&quot; 隔开</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="number">120.78</span><span class="number">.198</span><span class="number">.32</span><span class="string">:9092</span></span><br><span class="line">    <span class="attr">listener:</span></span><br><span class="line">      <span class="comment"># 当每一条消息被消费者监听器（ListenerConsumer）处理后才提交(record)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后提交(batch)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间⼤于 TIME 时提交(time)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后，被处理 record 数量⼤于等于COUNT时提交(count)</span></span><br><span class="line">      <span class="comment"># TIME |	COUNT	有⼀个条件满⾜时提交(count_time)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后, ⼿动调⽤ Acknowledgment.acknowledge() 后提交(manual)</span></span><br><span class="line">      <span class="comment"># ⼿动调⽤ Acknowledgment.acknowledge() 后⽴即提交，⼀般使⽤这种(manual_immediate)</span></span><br><span class="line">      <span class="attr">ack-mode:</span> <span class="string">manual_immediate</span></span><br></pre></td></tr></table></figure><h3 id="4、完整配置"><a href="#4、完整配置" class="headerlink" title="4、完整配置"></a>4、完整配置</h3><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">server:</span></span><br><span class="line">  <span class="attr">port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">kafka:</span></span><br><span class="line">    <span class="comment"># kafka-server 的地址，如果是集群，那么 ip:port 之间使用 &quot;,&quot; 隔开</span></span><br><span class="line">    <span class="attr">bootstrap-servers:</span> <span class="number">120.78</span><span class="number">.198</span><span class="number">.32</span><span class="string">:9092</span></span><br><span class="line">    <span class="comment"># 生产者</span></span><br><span class="line">    <span class="attr">producer:</span></span><br><span class="line">      <span class="comment"># 设置一个大于 0 的值，则客户端会将发送失败的记录重新发送</span></span><br><span class="line">      <span class="attr">retries:</span> <span class="number">3</span></span><br><span class="line">      <span class="comment"># 生产者一次从本地缓冲区中取出 batch-size 大小的数据发送给 Kafka</span></span><br><span class="line">      <span class="attr">batch-size:</span> <span class="number">16384</span></span><br><span class="line">      <span class="comment"># 设置 acks 为 1， ⾄少要等待 leader 已经成功将数据写⼊本地 log，但是不需要等待所有 follower 是否成功写⼊。就可以继续发送下⼀条消息。</span></span><br><span class="line">      <span class="attr">acks:</span> <span class="number">1</span></span><br><span class="line">      <span class="comment"># 指定消息 key 与消息体的编解码方式</span></span><br><span class="line">      <span class="attr">key-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="attr">value-serializer:</span> <span class="string">org.apache.kafka.common.serialization.StringSerializer</span></span><br><span class="line">      <span class="comment"># 生产者本地缓冲区，默认为 32m</span></span><br><span class="line">      <span class="attr">buffer-memory:</span> <span class="number">33554432</span></span><br><span class="line">    <span class="attr">consumer:</span></span><br><span class="line">      <span class="comment"># 指定消费组 id</span></span><br><span class="line">      <span class="attr">group-id:</span> <span class="string">default-group</span></span><br><span class="line">      <span class="comment"># 关闭自动提交</span></span><br><span class="line">      <span class="attr">enable-auto-commit:</span> <span class="literal">false</span></span><br><span class="line">      <span class="comment"># 当出现一个新的消费组或者需要传入 offset 但 offset 为空时，指定其从头开始消费</span></span><br><span class="line">      <span class="comment"># 但不是每次都是从头开始消费</span></span><br><span class="line">      <span class="attr">auto-offset-reset:</span> <span class="string">earliest</span></span><br><span class="line">      <span class="attr">key-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="attr">value-deserializer:</span> <span class="string">org.apache.kafka.common.serialization.StringDeserializer</span></span><br><span class="line">      <span class="comment"># 消费者一次从 Broker 最多拉取 500 条消息</span></span><br><span class="line">      <span class="attr">max-poll-records:</span> <span class="number">500</span></span><br><span class="line">    <span class="attr">listener:</span></span><br><span class="line">      <span class="comment"># 当每一条消息被消费者监听器（ListenerConsumer）处理后才提交(record)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后提交(batch)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间⼤于 TIME 时提交(time)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后，被处理 record 数量⼤于等于COUNT时提交(count)</span></span><br><span class="line">      <span class="comment"># TIME |	COUNT	有⼀个条件满⾜时提交(count_time)</span></span><br><span class="line">      <span class="comment"># 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后, ⼿动调⽤ Acknowledgment.acknowledge() 后提交(manual)</span></span><br><span class="line">      <span class="comment"># ⼿动调⽤ Acknowledgment.acknowledge() 后⽴即提交，⼀般使⽤这种(manual_immediate)</span></span><br><span class="line">      <span class="attr">ack-mode:</span> <span class="string">manual_immediate</span></span><br></pre></td></tr></table></figure><h3 id="5、Kafka-消费模式介绍"><a href="#5、Kafka-消费模式介绍" class="headerlink" title="5、Kafka 消费模式介绍"></a>5、Kafka 消费模式介绍</h3><blockquote><p>Kafka 支持多种消费模式，消费模式在 <code>org.springframework.kafka.listener.ContainerProperties.AckMode</code> 枚举中，这是一个静态内部枚举，我们可以查看这个类中的详细信息</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * The offset commit behavior enumeration.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">enum</span> <span class="title">AckMode</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Commit after each record is processed by the listener.</span></span><br><span class="line"><span class="comment">     * 当每一条消息被消费者监听器（ListenerConsumer）处理后才提交(record)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    RECORD,</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Commit whatever has already been processed before the next poll.</span></span><br><span class="line"><span class="comment">     * 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后提交(batch)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    BATCH,</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Commit pending updates after</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> ContainerProperties#setAckTime(long) ackTime&#125; has elapsed.</span></span><br><span class="line"><span class="comment">     * 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后，距离上次提交时间⼤于 TIME 时提交(time)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    TIME,</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Commit pending updates after</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> ContainerProperties#setAckCount(int) ackCount&#125; has been</span></span><br><span class="line"><span class="comment">     * exceeded.</span></span><br><span class="line"><span class="comment">     * 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后，被处理 record 数量⼤于等于COUNT时提交(count)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    COUNT,</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * Commit pending updates after</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> ContainerProperties#setAckCount(int) ackCount&#125; has been</span></span><br><span class="line"><span class="comment">     * exceeded or after &#123;<span class="doctag">@link</span> ContainerProperties#setAckTime(long)</span></span><br><span class="line"><span class="comment">     * ackTime&#125; has elapsed.</span></span><br><span class="line"><span class="comment">     * TIME |	COUNT	有⼀个条件满⾜时提交(count_time)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    COUNT_TIME,</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * User takes responsibility for acks using an</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> AcknowledgingMessageListener&#125;.</span></span><br><span class="line"><span class="comment">     * 当每⼀批 poll() 的数据被消费者监听器（ListenerConsumer）处理之后, ⼿动调⽤ Acknowledgment.acknowledge() 后提交(manual)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    MANUAL,</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * User takes responsibility for acks using an</span></span><br><span class="line"><span class="comment">     * &#123;<span class="doctag">@link</span> AcknowledgingMessageListener&#125;. The consumer</span></span><br><span class="line"><span class="comment">     * immediately processes the commit.</span></span><br><span class="line"><span class="comment">     * ⼿动调⽤ Acknowledgment.acknowledge() 后⽴即提交，⼀般使⽤这种(manual_immediate)</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    MANUAL_IMMEDIATE,</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p><code>MANUAL</code> 和 <code>MANUAL_IMMEDIATE</code> 的区别</p></blockquote><ul><li>MANUAL：只有当 poll 下来的消息全部处理完后，再调用 Acknowledgment.acknowledge() 进行批量提交</li><li>MANUAL_IMMEDIATE：当调用 Acknowledgment.acknowledge() 后立即提交，处理一条提交一条</li></ul><h2 id="5-3、消息生产者"><a href="#5-3、消息生产者" class="headerlink" title="5.3、消息生产者"></a>5.3、消息生产者</h2><ul><li>指定要发送的分区</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br></pre></td></tr></table></figure><ul><li>在生产者中注入 KafkaTemplate 对象</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br></pre></td></tr></table></figure><ul><li><p>创建消息，并调用 KafkaTemplate 对象的 send 方法发送消息</p></li><li><p>全部代码</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@RestController</span></span><br><span class="line"><span class="meta">@RequestMapping(&quot;message&quot;)</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaProducerController</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String TOPIC_NAME = <span class="string">&quot;my-replicated-topic&quot;</span>;</span><br><span class="line">    <span class="meta">@Autowired</span></span><br><span class="line">    <span class="keyword">private</span> KafkaTemplate&lt;String, String&gt; kafkaTemplate;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@GetMapping(&quot;send&quot;)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">send</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Product product = <span class="keyword">new</span> Product(<span class="string">&quot;100&quot;</span>, <span class="string">&quot;测试商品-100&quot;</span>);</span><br><span class="line">        <span class="comment">// 在这里进行消息发送，指定主题、分区、key和消息记录</span></span><br><span class="line">        kafkaTemplate.send(TOPIC_NAME, <span class="number">0</span>, product.getProductId(), JSON.toJSONString(product));</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;send success!&quot;</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-4、消息消费者"><a href="#5-4、消息消费者" class="headerlink" title="5.4、消息消费者"></a>5.4、消息消费者</h2><ul><li>创建一个 Consumer 类，需要将这个类交给 Spring 容器管理，在类上添加 <code>@Component</code> 注解</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>创建一个方法，在方法上添加 <code>@KafkaListener</code> 注解，在注解中指定监听的主题和消费组</li></ul><blockquote><p>如果在处理消息后没有进行提交，那么这条消息会被重复消费，当自动提交关闭时，需要我们使用 <code>ack.acknowledge();</code> 进行手动提交</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@KafkaListener(topics = KafkaProducerController.TOPIC_NAME, groupId = GROUP_ID)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenGroup</span> <span class="params">(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">    String value = record.value();</span><br><span class="line">    System.out.println(value);</span><br><span class="line">    System.out.println(record);</span><br><span class="line">    <span class="comment">// 手动提交 offset </span></span><br><span class="line">    ack.acknowledge();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>全部代码</li></ul><blockquote><p>实际上拿到的是多条记录，但每次只对一条记录进行处理</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Component</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">KafkaConsumer</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">final</span> String GROUP_ID = <span class="string">&quot;MyGroup&quot;</span>;</span><br><span class="line">    <span class="meta">@KafkaListener(topics = KafkaProducerController.TOPIC_NAME, groupId = GROUP_ID)</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenGroup</span> <span class="params">(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">        String value = record.value();</span><br><span class="line">        System.out.println(value);</span><br><span class="line">        System.out.println(record);</span><br><span class="line">        <span class="comment">// 手动提交 offset</span></span><br><span class="line">        ack.acknowledge();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><blockquote><p>也可以对多条记录进行处理</p></blockquote><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@KafkaListener(topics = KafkaProducerController.TOPIC_NAME, groupId = GROUP_ID)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenGroups</span> <span class="params">(ConsumerRecords&lt;String, String&gt; records, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 处理逻辑</span></span><br><span class="line">    <span class="comment">// 手动提交 offset</span></span><br><span class="line">    ack.acknowledge();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="5-5、测试"><a href="#5-5、测试" class="headerlink" title="5.5、测试"></a>5.5、测试</h2><blockquote><p>启动项目，访问 <code>http://localhost:8080/message/send</code> 接口，查看结果</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011230130.png" alt="image-20211011162449160"></p><blockquote><p>控制台中输出结果如下，可以看到消费者收到了生产者发送的消息</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;productId&quot;:&quot;100&quot;,&quot;productName&quot;:&quot;测试商品-100&quot;&#125;</span><br><span class="line">ConsumerRecord(topic &#x3D; my-replicated-topic, partition &#x3D; 0, leaderEpoch &#x3D; 0, offset &#x3D; 0, CreateTime &#x3D; 1633940677717, serialized key size &#x3D; 3, serialized value size &#x3D; 52, headers &#x3D; RecordHeaders(headers &#x3D; [], isReadOnly &#x3D; false), key &#x3D; 100, value &#x3D; &#123;&quot;productId&quot;:&quot;100&quot;,&quot;productName&quot;:&quot;测试商品-100&quot;&#125;)</span><br></pre></td></tr></table></figure><h2 id="5-6、消费者的详细配置"><a href="#5-6、消费者的详细配置" class="headerlink" title="5.6、消费者的详细配置"></a>5.6、消费者的详细配置</h2><blockquote><p>设置消费组、多 Topic 、指定分区、指定偏移量和设置消费者个数</p></blockquote><ul><li>concurrency 就是同组下的消费者的个数，就是并发消费数，建议小于等于分组总数</li></ul><blockquote><p>启动时， Kafka 会帮你创建出 concurrency 个这样的消费者。</p></blockquote><ul><li>topicPartitions 由多个 topicPartition 组成</li></ul><blockquote><p>topicPartition 中指定了这个消费者要消费的主题和主题中哪几个分区</p></blockquote><ol><li>对于 <code>@TopicPartition(topic = &quot;topic1&quot;, partitions = &#123;&quot;0&quot;, &quot;1&quot;&#125;)</code> ，我们希望这个消费者消费主题 <code>topic1</code> 的第一个和第二个分区</li><li>对于 <code>@TopicPartition(topic = &quot;topic2&quot;, partitions = &quot;0&quot;,partitionOffsets = @PartitionOffset(partition = &quot;1&quot;, initialOffset = &quot;100&quot;))</code>，我们希望这个消费者消费第一个分区和第二个分区，同时在消费第二个分区时，初始偏移量为 100</li></ol><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@KafkaListener(groupId = &quot;testGroup&quot;, topicPartitions = &#123;</span></span><br><span class="line"><span class="meta">        @TopicPartition(topic = &quot;topic1&quot;, partitions = &#123;&quot;0&quot;, &quot;1&quot;&#125;),</span></span><br><span class="line"><span class="meta">        @TopicPartition(topic = &quot;topic2&quot;, partitions = &quot;0&quot;, partitionOffsets = @PartitionOffset(partition = &quot;1&quot;, initialOffset = &quot;100&quot;))</span></span><br><span class="line"><span class="meta">&#125;, concurrency = &quot;3&quot;)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">listenGroupPro</span><span class="params">(ConsumerRecord&lt;String, String&gt; record, Acknowledgment ack)</span> </span>&#123;</span><br><span class="line">    String value = record.value();</span><br><span class="line">    System.out.println(value);</span><br><span class="line">    System.out.println(record);</span><br><span class="line">    <span class="comment">// 手动提交 offset</span></span><br><span class="line">    ack.acknowledge();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="六、Kafka-及集群-Controller-、-Rebalance-和-HW"><a href="#六、Kafka-及集群-Controller-、-Rebalance-和-HW" class="headerlink" title="六、Kafka 及集群 Controller 、 Rebalance 和 HW"></a>六、Kafka 及集群 Controller 、 Rebalance 和 HW</h1><h2 id="6-1、Controller"><a href="#6-1、Controller" class="headerlink" title="6.1、Controller"></a>6.1、Controller</h2><blockquote><p>回到这张图，在下面的 Kafka 集群中，我们创建了两个分区、三个副本</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011230151.png" alt="image-20211011171119093"></p><blockquote><p>在众多副本中，存在一个副本会作为 leader ，这个副本负责数据的读写与同步，而其他的副本作为其的 follower 存在，在 leader 挂掉后，新 leader 会从处于 Isr 列表中的剩下 follower 之中选举产生。</p><p>controller 就负责这个新 follower 的选举和产生。</p></blockquote><h3 id="1、概念"><a href="#1、概念" class="headerlink" title="1、概念"></a>1、概念</h3><blockquote><p><strong>Kafka 集群中的 Broker 会在 ZK 中创建临时序号节点，序号最小的节点（最先创建的节点）将作为 Kafka 集群中的 controller</strong> ，负责管理整个集群中的所有分区和所有副本的状态。</p></blockquote><h3 id="2、职责"><a href="#2、职责" class="headerlink" title="2、职责"></a>2、职责</h3><ul><li>当某个分区的 Leader 副本发生故障时，<strong>由 controller 负责为该分区选举产生新的 Leader 副本</strong></li><li>当检测到某个分区的 ISR 集合发生变化时，<strong>由 controller 负责通知所有的 broker 更新其元数据信息</strong></li><li>当使⽤ <code>kafka-topics.sh</code> 脚本为某个 <code>topic</code> 增加分区数量时，同样还是<strong>由 controller 负责让新分区被其他节点感知到</strong></li></ul><h2 id="6-2、Rebalance-机制"><a href="#6-2、Rebalance-机制" class="headerlink" title="6.2、Rebalance 机制"></a>6.2、Rebalance 机制</h2><h3 id="1、触发前提"><a href="#1、触发前提" class="headerlink" title="1、触发前提"></a>1、触发前提</h3><blockquote><p><strong>消费者没有指明分区消费</strong></p></blockquote><h3 id="2、是什么"><a href="#2、是什么" class="headerlink" title="2、是什么?"></a>2、是什么?</h3><blockquote><p>重平衡是其实是一个协议，<strong>它规定了如何让消费者组下的所有消费者来分配 Topic 中的每一个分区</strong>。</p><p>比如说一个 Topic 中有 100 个分区，一个消费组中有 20 个消费者，在协调者（<code>Group Coordinator</code>）下的控制下让组内的每一个消费者分配到 5 个分区，这个分配的过程就是重平衡。</p><p>当消费组中消费者和分区的关系发生变化时，那么就会触发 <code>rebalance</code> 机制。</p></blockquote><h3 id="3、触发条件"><a href="#3、触发条件" class="headerlink" title="3、触发条件"></a>3、触发条件</h3><blockquote><p>重平衡的触发条件主要有三个：</p></blockquote><ul><li><strong>消费者组内成员发生变更，这个变更包括了增加和减少消费者</strong></li></ul><blockquote><p>注意：这里的减少很大可能是被动的，即消费者崩溃退出，这种情况是最常见的情况</p></blockquote><ul><li>主题的分区数发生变化，Kafka 目前只支持增加分区，当增加分区时会触发重平衡</li><li>订阅的主题数发生变化</li></ul><blockquote><p>当消费者组使用正则表达式订阅主题时，此时可能新建的主题符合正则规则，那么又会触发重平衡。</p></blockquote><h3 id="4、-Rebalance-机制带来的问题"><a href="#4、-Rebalance-机制带来的问题" class="headerlink" title="4、 Rebalance 机制带来的问题"></a>4、 Rebalance 机制带来的问题</h3><blockquote><p>在 Rebalance 过程中，消费者无法从 Kafka 中消费消息，这对 Kafka 的 TPS 影响极大，且 Rebalance 的时间会随着 Kafka 集群节点的增多而变长，在 Rebalance 时间内 Kafka 几乎处于不可用状态。</p><p>所以在实际环境中应该尽量避免 Rebalance 的发生。</p></blockquote><h3 id="5、消费者消费分区的三种策略"><a href="#5、消费者消费分区的三种策略" class="headerlink" title="5、消费者消费分区的三种策略"></a>5、消费者消费分区的三种策略</h3><blockquote><p>在触发 Rebalance 机制之前，消费者消费哪个分区有三种策略</p></blockquote><ul><li><p><code>range</code> ： 通过公式来计算某个消费者消费哪个分区</p></li><li><p><code>轮询</code> ：消费组中的消费者轮流消费</p></li><li><p><code>sticky</code> ： 在触发 rebalance 后，在消费者消费的原分区不变的基础上进行调整</p></li></ul><blockquote><p>如果这个策略没有开启，那么就要进行所有分区的重新分配，建议开启。</p></blockquote><h2 id="6-3、HW-与-LEO"><a href="#6-3、HW-与-LEO" class="headerlink" title="6.3、HW 与 LEO"></a>6.3、HW 与 LEO</h2><h3 id="1、基本概念"><a href="#1、基本概念" class="headerlink" title="1、基本概念"></a>1、基本概念</h3><ul><li>Base Offset</li></ul><blockquote><p><strong>起始偏移量，即副本中第一条消息的偏移量</strong>，如下图，这里的起始位移是 0，如果一个日志文件写满 1G 后（默认 1G 后会 log rolling），这个起始位移就不是 0 开始了。</p></blockquote><ul><li>HW（<code>high watermark</code>）</li></ul><blockquote><p>副本的高水位值，replica 中 Leader 副本和 Follower 副本都会有这个值，通过它可以得知副本中已提交或者已备份消息的范围， Leader 副本中的 HW ，决定了消费者能消费的最新消息能到哪个 offset 。</p><p>如下图所示，HW 值为 8 ，代表 offset 为 [0,8] 的 9 条消息都能被消费到，它们是对消费者可见的，而 [9,12] 这四条消息由于未提交，所以对消费者不可见。</p></blockquote><ul><li>LEO （<code>log end offset</code>）</li></ul><blockquote><p><strong>日志末端位移，代表日志文件中下一条待写入消息的 offset ，这个 offset 实际上是没有消息的</strong></p><p>不管是 Leader 副本还是 Follower 副本都有 LEO ，当 Leader 副本收到生产者的一条消息时，LEO 通常会自增 1 ，而 Follower 副本需要从 Leader 副本同步到数据后，才能增加它的 LEO 。</p><p>最后 Leader 副本会比较自己的 LEO 和满足条件的 Follower 副本上的 LEO ，然后选取两者中较小值作为新的 HW ，来更新自己的 HW 值。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011230226.png" alt="image-20211011194242472"></p><h3 id="2、为什么对于新写入的消息不能立即消费？"><a href="#2、为什么对于新写入的消息不能立即消费？" class="headerlink" title="2、为什么对于新写入的消息不能立即消费？"></a>2、为什么对于新写入的消息不能立即消费？</h3><blockquote><p>对于新写入到 Leader 副本的消息，不能直接交给 Consumer 进行消费，<strong>Leader 会等待该消息被所有 Isr 列表中的副本同步后，再更新 HW ，此时消息才能被 Consumer 进行消费。</strong></p><p><strong>这样就保证了如果 Leader 所在的 Broker 挂掉，该消息仍然可以从新选举的 Leader 中获取</strong>，可以有效地防止消息丢失。</p></blockquote><h1 id="七、Kafka-相关问题"><a href="#七、Kafka-相关问题" class="headerlink" title="七、Kafka 相关问题"></a>七、Kafka 相关问题</h1><h2 id="7-1、如何防止消息丢失"><a href="#7-1、如何防止消息丢失" class="headerlink" title="7.1、如何防止消息丢失"></a>7.1、如何防止消息丢失</h2><h3 id="1、生产者"><a href="#1、生产者" class="headerlink" title="1、生产者"></a>1、生产者</h3><ul><li>将 <code>acks</code> 设置 <code>1</code> 或者 <code>-1 / all</code> 可以有效防止消息丢失</li><li>如果要做到 <code>99.9999%</code> ，那么可以将 <code>acks</code> 设置为 all ，然后将 <code>min.insync.replicas</code> 设置为分区备份数</li></ul><blockquote><p>这样做，只有等待上一条消息被所有副本同步完后，才能发送下一条消息。</p></blockquote><h3 id="2、消费者"><a href="#2、消费者" class="headerlink" title="2、消费者"></a>2、消费者</h3><blockquote><p>将自动提交改为手动提交</p></blockquote><h2 id="7-2、如何防止消息的重复消费"><a href="#7-2、如何防止消息的重复消费" class="headerlink" title="7.2、如何防止消息的重复消费"></a>7.2、如何防止消息的重复消费</h2><blockquote><p>在生产环境中，可能出现以下情况</p></blockquote><ol><li>生产者向 Kafka 集群中投递消息，Kafka 已经接收到了这条消息，并返回给发送者一个 ack</li><li>此时因为网络动荡，导致生产者没有收到这个 ack ，所以生产者启动了重发机制，再次发送一条消息到 Kafka 中，这样就导致了 Kafka 收到了两条一模一样的消息</li></ol><blockquote><p>如果为了消息的不重复消费，⽽把⽣产端的重试机制关闭、消费端的⼿动提交改成⾃动提交，这样反⽽会出现消息丢失，那么<strong>可以直接在防治消息丢失的⼿段上再加上消费消息时的幂等性保证，就能解决消息的重复消费问题</strong>。</p></blockquote><ul><li>使用 MySQL 插入业务 id 作为主键，主键是唯一的，所以一次只能插入一条</li><li>使用 Redis 或者 Zookeeper 的分布式锁，以业务 id 为锁，保证只有一条记录能够创建成功</li><li>构建幂等表保证幂等性</li></ul><h2 id="7-3、如何做到顺序消费"><a href="#7-3、如何做到顺序消费" class="headerlink" title="7.3、如何做到顺序消费"></a>7.3、如何做到顺序消费</h2><h3 id="1、生产者-1"><a href="#1、生产者-1" class="headerlink" title="1、生产者"></a>1、生产者</h3><blockquote><p>在发送时不能将 <code>ack</code> 设置为 0 ，同时需要关闭重试，并使用同步发送，等到发送成功后再发送下一条消息，<strong>确保消息是顺序发送的。</strong></p></blockquote><h3 id="2、消费者-1"><a href="#2、消费者-1" class="headerlink" title="2、消费者"></a>2、消费者</h3><blockquote><p><strong>消息发送到一个分区中，同时只能有一个消费组里的消费者来接收消息</strong>，因此，Kafka 的顺序消费会牺牲掉一部分性能。</p></blockquote><h2 id="7-4、如何解决消息积压问题"><a href="#7-4、如何解决消息积压问题" class="headerlink" title="7.4、如何解决消息积压问题"></a>7.4、如何解决消息积压问题</h2><h3 id="1、问题说明"><a href="#1、问题说明" class="headerlink" title="1、问题说明"></a>1、问题说明</h3><blockquote><p>消息的消费者的消费速度远远赶不上生产者的生产消息的速度，导致 <code>Kafka</code> 中有大量的数据没有被消费，随着没有被消费的数据堆积越来越多，消费者寻址的性能会越来越差，最后导致整个 <code>Kafka</code> 对外提供服务的性能越来越差，从而拖垮其他服务，造成服务雪崩</p></blockquote><h3 id="2、解决方案"><a href="#2、解决方案" class="headerlink" title="2、解决方案"></a>2、解决方案</h3><blockquote><p>消息积压会导致很多问题，⽐如磁盘被打满、⽣产端发消息导致 <code>Kafka</code> 性能过慢，就容易出现服务雪崩，就需要有相应的⼿段：</p></blockquote><ul><li>⽅案⼀：在⼀个消费者中启动多个线程，让多个线程同时消费（提升⼀个消费者的消费能⼒），充分利用机器的性能。</li><li>⽅案⼆：如果⽅案⼀还不够的话，这个时候可以启动多个消费者，多个消费者部署在不同的服务器上。其实多个消费者部署在同⼀服务器上也可以提⾼消费能⼒（充分利⽤服务器的 <code>cpu</code> 资源）。</li><li>⽅案三：让⼀个消费者去把收到的消息往另外⼀个 <code>topic</code> 上发，另⼀个 <code>topic</code> 设置多个分区和多个消费者 ，进⾏具体的业务消费。</li></ul><h2 id="7-5、Kafka-为什么快？"><a href="#7-5、Kafka-为什么快？" class="headerlink" title="7.5、Kafka 为什么快？"></a>7.5、Kafka 为什么快？</h2><blockquote><p>Kafka 主要做了 4 个优化</p></blockquote><ul><li>磁盘顺序读写</li><li>页缓存</li><li>零拷贝</li><li>批量操作</li></ul><h3 id="1、磁盘顺序读写"><a href="#1、磁盘顺序读写" class="headerlink" title="1、磁盘顺序读写"></a>1、磁盘顺序读写</h3><blockquote><p>Kafka 进行的第一个优化就是<strong>磁盘顺序读写</strong>，在传统认知中，磁盘的读取效率应该远低于内存，但 2019 年 <a target="_blank" rel="noopener" href="https://queue.acm.org/detail.cfm?id=1563874">ACM 机构的调研</a>中表明，磁盘顺序读写的效率虽然远不及内存顺序读写的效率，但前者的效率是要略高于内存随机读写的效率的。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011230251.png" alt="image-20211011211150388"></p><blockquote><p>上面的文章在一定程度上指导了 Kafka 后面的设计，<strong>Kafka 基于顺序读写实现高性能</strong>。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011230308.png" alt="image-20211011211432085"></p><blockquote><p>每一个分区对应一个日志，新消息到来后在日志后面进行顺序追加，而对于已经消费过的消息，Kafka 并不对其进行实时地数据删除，而是基于某些策略（存放时间、日志大小等）来对这些已经消费过的消息进行统一批量删除。</p></blockquote><h3 id="2、页缓存"><a href="#2、页缓存" class="headerlink" title="2、页缓存"></a>2、页缓存</h3><blockquote><p>虽然 Kafka 是基于 Scala 编写，<strong>必须依赖 JVM 运行的应用</strong>，但 <strong>Kafka 中的消息在处理投递过程中并不直接通过 JVM</strong> ，而是<strong>直接使用使用操作系统的页缓存特性来提高处理速度</strong>。</p><p>这样做的好处有什么?</p></blockquote><ul><li>避免了 JVM GC 带来的性能损耗</li><li>Kafka 采用字节紧密存储，避免产生对象，进一步提高了空间利用率</li></ul><h3 id="3、零拷贝"><a href="#3、零拷贝" class="headerlink" title="3、零拷贝"></a>3、零拷贝</h3><blockquote><p>在进行数据传递过程中，不再发生内核态 - 用户态的转变，而是直接将页缓存中的数据拷贝到 Socket 缓冲区。</p><p><strong>这里的零拷贝不是说不发生复制，而是说不再发生与用户态发生的数据拷贝</strong>。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211011230350.png" alt="image-20211011212557504"></p><h3 id="4、批处理"><a href="#4、批处理" class="headerlink" title="4、批处理"></a>4、批处理</h3><ul><li>Kafka 在生产者端设置有本地缓存，同时会采用线程异步将数据批量提交到 Broker 的方式提高性能</li><li>Kafka 提供了许多批处理的 API ，可以让我们对数据进行统一的压缩和合并，让数据通过更小的数据包来进行网络传输，提高了性能</li></ul><h2 id="7-6、Kafka-的幂等性"><a href="#7-6、Kafka-的幂等性" class="headerlink" title="7.6、Kafka 的幂等性"></a>7.6、Kafka 的幂等性</h2><h3 id="1、Kafka-生产者幂等性"><a href="#1、Kafka-生产者幂等性" class="headerlink" title="1、Kafka 生产者幂等性"></a>1、Kafka 生产者幂等性</h3><blockquote><p>在生产者向 Kafka 发送消息过程中，可能会出现这种情况，Kafka 集群已经接收到了这条消息，同时已经将其写入日志了，但在向生产者返回 <code>ack</code> 的过程中由于某些原因 <code>ack</code> 丢失导致生产者没有接收到 <code>ack</code>，此时生产者会启动重试机制，如果 Kafka 无法保证幂等性，那么可能在 partition 中会保存多条一模一样的消息。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233223.png" alt="image-20211012160455192"></p><h3 id="2、生产者配置幂等性"><a href="#2、生产者配置幂等性" class="headerlink" title="2、生产者配置幂等性"></a>2、生产者配置幂等性</h3><ul><li>只需要在生产者端添加一个配置即可</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">properties.put(ENABLE_IDEMPOTENCE_CONFIG, <span class="keyword">true</span>)</span><br></pre></td></tr></table></figure><h3 id="3、Kafka-保证幂等性的原理"><a href="#3、Kafka-保证幂等性的原理" class="headerlink" title="3、Kafka 保证幂等性的原理"></a>3、Kafka 保证幂等性的原理</h3><blockquote><p>为实现生产者的幂等性， Kafka 引入了 Producer ID （PID）和 Sequence Number 两个概念。</p></blockquote><ul><li>PID：<strong>每个 Producer 在初始化时，都会分配一个唯一的 PID，这个 PID 对用户来说，是透明的</strong>。</li><li>Sequence Number：针对每个生产者（对应 PID ）发送到指定主题分区的消息都对应一个从 0 开始递增的 Sequence Number。</li></ul><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233242.png" alt="image-20211012161500813"></p><ul><li>在生产者向 Broker 发送消息时，**会将自己的 PID 和此条消息的序列号一起发送给 Broker **</li><li>Broker 接收到消息后，将消息、 PID 和 序列号一起保存</li><li>Broker 向生产者返回一个发送成功的 ACK ，但这个 ACK 在返回过程中因为某个原因丢失了，没有送到生产者手上</li><li>生产者收不到 ACK ，以为消息发送失败，所以启动重试机制，在发送的消息中同样带上自己的 PID 和消息的序列号（PID: 250, seq: 0）</li><li>Broker 接收到重发消息后，检查这条消息的 PID 和 seq ，发现这条消息的 seq &lt;= 分区中该 PID 对应的 seq ，所以不进行保存</li><li>Broker 向生产者发送一个 ACK</li></ul><h2 id="7-7、Kafka-的数据存储形式"><a href="#7-7、Kafka-的数据存储形式" class="headerlink" title="7.7、Kafka 的数据存储形式"></a>7.7、Kafka 的数据存储形式</h2><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233317.png" alt="image-20211012164238789"></p><ul><li>一个 <code>Topic</code> 由多个分区组成</li><li>一个分区（partition）由多个 <code>segment</code> （段）组成</li><li>一个 <code>segment</code>（段）由多个文件组成（<code>log、index、timeindex</code>）</li></ul><h3 id="1、存储日志"><a href="#1、存储日志" class="headerlink" title="1、存储日志"></a>1、存储日志</h3><blockquote><p>我们先来看看 Kafka 中的数据是如何在磁盘中进行存储的，进入 Kafka 安装目录下的 <code>data/kafka-logs</code> 文件夹中，可以看到以下命名的文件夹，这些就是 Kafka 中的消息在磁盘中的存储位置，<strong>文件夹以 <code>主题名</code> - <code>分区 ID</code> 的格式命名</strong>，下面就是 Kafka 中 <code>my-replicated-topic</code> 主题下两个分区的物理存储目录。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233340.png" alt="image-20211012164657651"></p><blockquote><p>进入其中一个目录，查看目录下的文件，可以看到以下几个文件，我们对这些文件进行一一介绍</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233354.png" alt="image-20211012164920515"></p><table><thead><tr><th align="center">文件名</th><th align="center">描述</th></tr></thead><tbody><tr><td align="center">00000000000000000000.index</td><td align="center">索引文件，根据 offset 查找数据时就是根据该索引文件来操作的</td></tr><tr><td align="center">00000000000000000000.log</td><td align="center">日志数据文件，真正存放消息数据的文件</td></tr><tr><td align="center">00000000000000000000.timeindex</td><td align="center">时间索引文件</td></tr><tr><td align="center">00000000000000000001.snapshot</td><td align="center">快照文件</td></tr><tr><td align="center">leader-epoch-checkpoint</td><td align="center">持久化每个 partition leader 的 LEO</td></tr></tbody></table><ul><li><strong>segment 是几个文件名一致的文件的集合</strong>。在一个分区文件夹下 （<code>my-replicated-topic-0</code> ）可能可以看到前缀一样，但文件类型的几个文件，**这种前缀相同，但文件类型不同的几个文件组合起来，就称之为一个 segment **</li></ul><blockquote><p>假设说一个分区文件夹下，有 6 个这样的文件（简化它们的文件名），分别为（a.index、a.log、a.timeindex、b.index、b.log、b.timeindex），那么我们可以称这个分区中有两个 segment ，即 a 段（a.index、a.log、a.timeindex）和 b 段（b.index、b.log、b.timeindex）。</p></blockquote><ul><li>每个 segment 文件的文件名固定是 20 位数字，通常为 0…….xx.abc ，文件名代表这个 segment 的起始偏移量，因为每个分区的起始偏移量都是 0 ，所以分区的日志文件固定从 <code>0000000000000000000.log</code> 开始</li></ul><blockquote><p><code>00000000000000000097.log</code> 代表这个 segment 的第一条消息的偏移量为 97</p></blockquote><ul><li>默认的每个日志文件最大为「log.segment.bytes =1024 * 1024 * 1024」1G</li></ul><h3 id="2、写入消息"><a href="#2、写入消息" class="headerlink" title="2、写入消息"></a>2、写入消息</h3><ul><li>新的消息总是写入到最后一个 segment 的 log 文件中</li><li>如果该文件到达了指定的大小（默认为 1 GB），那么将滚动到一个新的文件中。</li></ul><h3 id="3、读取消息"><a href="#3、读取消息" class="headerlink" title="3、读取消息"></a>3、读取消息</h3><blockquote><p>日志段 segment 的引入方便了 Kafka 数据的查询和定位，Kafka 使用二分查找来查询消息。</p><p>Kafka 的日志段又分为活跃日志段和非活跃日志段，<strong>一个分区只能存在一个活跃日志段，而只有活跃日志段才可以被写入和读取，非活跃日志段只能被读取</strong>。</p></blockquote><ul><li>使用 Offset 查找消息</li></ul><blockquote><p>偏移量索引文件由 4 字节的相对位移（offset）和 4 字节的物理地址（position）组成。</p><p>在我们使用 kafka-run-class.sh 来查看 .index 文件后，我们可以看到以下内容</p></blockquote><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./kafka-run-class.sh kafka.tools.DumpLogSegments --files /usr/local/kafka-2.5.1/data/kafka-logs/my-replicated-topic-0/00000000000000000000.index</span><br></pre></td></tr></table></figure><blockquote><p>可以看到每一行都是 offset: xxx position: yyyy ，其中 offset 为相对偏移量，而 position 为物理地址</p><p>第一行的 offset: 12 position: 4423 代表相对偏移量从 0 - 12 的消息的物理地址在 0 - 4423</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233420.png" alt="image-20211012171900217"></p><blockquote><p>按上面的例子，如何找到 offset 为 60 的消息？</p></blockquote><ol><li>先根据 offset 找到对应的日志段，即 segment ，这里我们找到 00000000000000000000.index</li><li>通过二分法找到不大于 offset 的最大索引项，这里我们找到第二行（offset：24 position：8773）</li><li>到 00000000000000000000.log 文件中，从 position 为 8773 的位置开始顺序扫描，直到找到 offset 为 60 的消息</li></ol><ul><li>使用 TimeStamp 进行消息查找</li></ul><blockquote><p>时间戳索引文件是由 8 字节的时间戳和 4 字节的相对偏移量组成，dump 下来的文件格式如下</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233436.png" alt="image-20211012173318334"></p><ol><li>查找该时间戳应该在哪个 segment 中，将待查找的时间戳（timestamp）与各个日志分段中的最大时间戳（largestTimeStamp）逐一对比，直到找到不小于 timestamp 所对应的日志分段。</li><li>查找该日志分段的偏移量索引文件，查找该偏移量对应的 position</li><li>到 .log 文件中，从 position 开始扫描，直到找到待查找数据</li></ol><h3 id="4、删除消息"><a href="#4、删除消息" class="headerlink" title="4、删除消息"></a>4、删除消息</h3><ul><li>在消息被消费后，<strong>Kafka 不会实时删除已经被消费过的消息，而是定期清理，一次性删除一个 segment 的日志文件</strong></li><li>Kafka 的日志管理器会根据 Kafka 的配置，来决定哪些文件可以被删除。</li></ul><h2 id="7-8、Kafka-中数据清理（Log-Deletion）"><a href="#7-8、Kafka-中数据清理（Log-Deletion）" class="headerlink" title="7.8、Kafka 中数据清理（Log Deletion）"></a>7.8、Kafka 中数据清理（Log Deletion）</h2><blockquote><p><code>Kafka</code> 的<strong>消息存储在磁盘</strong>中，为了<strong>控制磁盘占用空间</strong>，<code>Kafka</code> <strong>需要不断地对过去的一些消息进行清理工作</strong>。</p><p>Kafka 的每个分区都有很多的日志文件，这样也是为了方便进行日志的清理。在Kafka 中，提供两种日志清理方式：</p></blockquote><ul><li>日志删除</li></ul><blockquote><p>按照指定的策略直接删除不符合条件的日志</p></blockquote><ul><li>日志压缩</li></ul><blockquote><p>按照消息的 Key 进行整合，如果遇到有相同 Key 但有不同 value 的消息，那么只保留最后一个版本</p></blockquote><h3 id="1、Kafka-数据清理配置"><a href="#1、Kafka-数据清理配置" class="headerlink" title="1、Kafka 数据清理配置"></a>1、Kafka 数据清理配置</h3><blockquote><p>在 Kafka 的 Broker 和 Topic 配置中</p></blockquote><table><thead><tr><th align="center">配置项</th><th align="center">配置值</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">log.cleaner.enable</td><td align="center">true（默认）</td><td align="center">是否开启日志清理功能</td></tr><tr><td align="center">log.cleanup.policy</td><td align="center">delete（默认）</td><td align="center">删除日志</td></tr><tr><td align="center">log.cleanup.policy</td><td align="center">compaction</td><td align="center">压缩日志</td></tr><tr><td align="center">log.cleanup.policy</td><td align="center">delete,compact</td><td align="center">同时支持压缩和删除</td></tr></tbody></table><h3 id="2、日志删除"><a href="#2、日志删除" class="headerlink" title="2、日志删除"></a>2、日志删除</h3><blockquote><p><strong>日志删除是以段为单位进行定期清理</strong></p></blockquote><ul><li>定时日志删除任务</li></ul><blockquote><p>Kafka 日志管理器中会有一个专门的日志删除任务来定期检测和删除不符合保留条件的日志分段文件，这个周期可以通过 <code>broker</code> 端参数 <code>log.retention.check.interval.ms</code> 来配置，默认值为 <code>300,000</code> ，即5分钟。当前日志分段的保留策略有3种：</p></blockquote><ol><li>基于时间的保留策略</li><li>基于日志大小的保留策略</li><li>基于日志起始偏移量的保留策略</li></ol><h3 id="3、基于时间的保留策略"><a href="#3、基于时间的保留策略" class="headerlink" title="3、基于时间的保留策略"></a>3、基于时间的保留策略</h3><blockquote><p>以下三种配置可以指定如果 Kafka 中的消息超过指定的阈值，就会将日志进行自动清理：</p></blockquote><ul><li><p>log.retention.hours</p></li><li><p>log.retention.minutes</p></li><li><p>log.retention.ms</p></li></ul><blockquote><p>其中，优先级 <code>log.retention.ms &gt; log.retention.minutes &gt; log.retention.hours</code>，默认情况下，在 broker 中配置为 <code>log.retention.hours = 168</code>，即日志默认保留 168 小时，相当于保留七天</p></blockquote><ul><li>删除日志分段时</li></ul><ol><li>从日志文件对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作</li><li>将日志分段文件添加上“.deleted”的后缀（也包括日志分段对应的索引文件）</li><li>Kafka 的后台定时任务会定期删除这些“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过 <code>file.delete.delay.ms</code> 参数来设置，默认值为60000，即1分钟。</li></ol><h3 id="4、基于日志大小的保留策略"><a href="#4、基于日志大小的保留策略" class="headerlink" title="4、基于日志大小的保留策略"></a>4、基于日志大小的保留策略</h3><ul><li>日志删除任务会检查当前日志的大小是否超过设定的阈值来寻找可删除的日志分段的文件集合。</li><li>可以通过 <code>broker</code> 端参数 <code>log.retention.bytes</code> 来配置，默认值为 <code>-1</code>，表示无穷大。如果超过该大小，会自动将超出部分删除。</li></ul><blockquote><p>注意：<code>log.retention.bytes</code> 配置的是日志文件的总大小，而不是单个的日志分段的大小，一个日志文件包含多个日志分段。</p></blockquote><h3 id="5、基于日志起始偏移量保留策略"><a href="#5、基于日志起始偏移量保留策略" class="headerlink" title="5、基于日志起始偏移量保留策略"></a>5、基于日志起始偏移量保留策略</h3><blockquote><p>每个 <code>segment</code> 日志都有它的起始偏移量，如果起始偏移量小于 logStartOffset，那么这些日志文件将会标记为删除。</p></blockquote><h3 id="6、日志压缩"><a href="#6、日志压缩" class="headerlink" title="6、日志压缩"></a>6、日志压缩</h3><blockquote><p>Log Compaction 是默认的日志删除之外的清理过时数据的方式。它会将相同的 key 对应的数据只保留一个版本。</p></blockquote><p><img src="/img/loading.gif" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012233455.png" alt="image-20211012191155887"></p><ul><li><strong>Log Compaction执行后，offset将不再连续，但依然可以查询Segment</strong></li><li>Log Compaction执行前后，日志分段中的每条消息偏移量保持不变。Log Compaction会生成一个新的Segment文件</li><li>Log Compaction是针对key的，在使用的时候注意每个消息的key不为空</li><li>基于 Log Compaction 可以保留key的最新更新，可以基于Log Compaction来恢复消费者的最新状态</li></ul></article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>Java应用学习（十一）-Kafka 学习笔记二</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://sutianxin.top/posts/2701940507.html">https://sutianxin.top/posts/2701940507.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a" style="display:inline-block;width:120px"><h>作者</h><div class="post-copyright-cc-info"><h>天昕</h></div></div><div class="post-copyright-c" style="display:inline-block;width:120px"><h>发布于</h><div class="post-copyright-cc-info"><h>2021-10-12</h></div></div><div class="post-copyright-u" style="display:inline-block;width:120px"><h>更新于</h><div class="post-copyright-cc-info"><h>2021-10-12</h></div></div><div class="post-copyright-c" style="display:inline-block;width:120px"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY 4.0" href="https://creativecommons.org/licenses/by/4.0/deed.zh">CC BY 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%F0%9F%93%AB%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/">📫消息队列</a></div><div class="post_share"><div class="social-share" data-image="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012235444.jpg" data-sites="wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/1637503980.html"><img class="prev-cover" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012234835.jpg" onerror='onerror=null,src="https://gitee.com/sutianxin/blogImage/raw/master/20210430103138.jpg"' alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Java应用学习（十）-Kafka 学习笔记一</div></div></a></div><div class="next-post pull-right"><a href="/posts/3469350670.html"><img class="next-cover" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012234202.jpg" onerror='onerror=null,src="https://gitee.com/sutianxin/blogImage/raw/master/20210430103138.jpg"' alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据结构与算法学习（十四）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/1637503980.html" title="Java应用学习（十）-Kafka 学习笔记一"><img class="cover" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/img/20211012234835.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-10-12</div><div class="title">Java应用学习（十）-Kafka 学习笔记一</div></div></a></div><div><a href="/posts/1158970579.html" title="Java应用学习（五）-RabbitMQ学习笔记（一）"><img class="cover" data-lazy-src="https://gitee.com/sutianxin/photo/raw/master/20210320231831.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-20</div><div class="title">Java应用学习（五）-RabbitMQ学习笔记（一）</div></div></a></div><div><a href="/posts/1280028895.html" title="Java应用学习（六）-RabbitMQ学习笔记（二）"><img class="cover" data-lazy-src="https://gitee.com/sutianxin/blogImage/raw/master/20210504190053.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2021-03-23</div><div class="title">Java应用学习（六）-RabbitMQ学习笔记（二）</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9B%9B%E3%80%81Kafka-%E7%9A%84-Java-%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">四、Kafka 的 Java 客户端</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-text">4.1、生产者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%BC%95%E5%85%A5%E4%BE%9D%E8%B5%96"><span class="toc-text">1、引入依赖</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0"><span class="toc-text">2、基本实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%8F%91%E9%80%81%E6%B6%88%E6%81%AF%E5%88%B0%E6%8C%87%E5%AE%9A%E5%88%86%E5%8C%BA%E4%B8%8A"><span class="toc-text">3、发送消息到指定分区上</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%90%8C%E6%AD%A5%E5%8F%91%E9%80%81"><span class="toc-text">4、同步发送</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E5%BC%82%E6%AD%A5%E5%8F%91%E6%B6%88%E6%81%AF"><span class="toc-text">5、异步发消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E5%85%B3%E4%BA%8E%E7%94%9F%E4%BA%A7%E8%80%85%E7%9A%84-ACK-%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE"><span class="toc-text">6、关于生产者的 ACK 参数配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9%E3%80%81%E5%85%B6%E4%BB%96%E7%BB%86%E8%8A%82"><span class="toc-text">9、其他细节</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-text">4.2、消费者</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%9F%BA%E6%9C%AC%E5%AE%9E%E7%8E%B0"><span class="toc-text">1、基本实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81Kafka-%E6%B6%88%E8%B4%B9%E8%80%85-offset-%E7%9A%84%E8%87%AA%E5%8A%A8%E6%8F%90%E4%BA%A4%E5%92%8C%E6%89%8B%E5%8A%A8%E6%8F%90%E4%BA%A4"><span class="toc-text">2、Kafka 消费者 offset 的自动提交和手动提交</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85-poll-%E6%B6%88%E6%81%AF%E7%9A%84%E8%BF%87%E7%A8%8B"><span class="toc-text">3、消费者 poll 消息的过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E6%8C%87%E5%AE%9A%E4%B8%BB%E9%A2%98%E7%9A%84%E5%88%86%E5%8C%BA%E8%BF%9B%E8%A1%8C%E6%B6%88%E8%B4%B9"><span class="toc-text">4、消费者指定主题的分区进行消费</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%B6%88%E6%81%AF%E7%9A%84%E5%9B%9E%E6%BA%AF%E6%B6%88%E8%B4%B9"><span class="toc-text">5、消息的回溯消费</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%8C%87%E5%AE%9A%E6%96%B0%E6%B6%88%E8%B4%B9%E7%BB%84%E7%9A%84%E6%B6%88%E8%B4%B9%E5%81%8F%E7%A7%BB%E9%87%8F"><span class="toc-text">6、指定新消费组的消费偏移量</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%94%E3%80%81Spring-Boot-%E4%BD%BF%E7%94%A8-Kafka"><span class="toc-text">五、Spring Boot 使用 Kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1%E3%80%81%E5%89%8D%E6%9C%9F%E5%87%86%E5%A4%87"><span class="toc-text">5.1、前期准备</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2%E3%80%81%E5%88%9B%E5%BB%BA%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">5.2、创建配置文件</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE"><span class="toc-text">1、生产者相关配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E7%9B%B8%E5%85%B3%E9%85%8D%E7%BD%AE"><span class="toc-text">2、消费者相关配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E9%A2%9D%E5%A4%96%E9%85%8D%E7%BD%AE"><span class="toc-text">3、额外配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%AE%8C%E6%95%B4%E9%85%8D%E7%BD%AE"><span class="toc-text">4、完整配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81Kafka-%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F%E4%BB%8B%E7%BB%8D"><span class="toc-text">5、Kafka 消费模式介绍</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3%E3%80%81%E6%B6%88%E6%81%AF%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-text">5.3、消息生产者</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4%E3%80%81%E6%B6%88%E6%81%AF%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-text">5.4、消息消费者</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-5%E3%80%81%E6%B5%8B%E8%AF%95"><span class="toc-text">5.5、测试</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-6%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E7%9A%84%E8%AF%A6%E7%BB%86%E9%85%8D%E7%BD%AE"><span class="toc-text">5.6、消费者的详细配置</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%AD%E3%80%81Kafka-%E5%8F%8A%E9%9B%86%E7%BE%A4-Controller-%E3%80%81-Rebalance-%E5%92%8C-HW"><span class="toc-text">六、Kafka 及集群 Controller 、 Rebalance 和 HW</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1%E3%80%81Controller"><span class="toc-text">6.1、Controller</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E6%A6%82%E5%BF%B5"><span class="toc-text">1、概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E8%81%8C%E8%B4%A3"><span class="toc-text">2、职责</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2%E3%80%81Rebalance-%E6%9C%BA%E5%88%B6"><span class="toc-text">6.2、Rebalance 机制</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E8%A7%A6%E5%8F%91%E5%89%8D%E6%8F%90"><span class="toc-text">1、触发前提</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-text">2、是什么?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E8%A7%A6%E5%8F%91%E6%9D%A1%E4%BB%B6"><span class="toc-text">3、触发条件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81-Rebalance-%E6%9C%BA%E5%88%B6%E5%B8%A6%E6%9D%A5%E7%9A%84%E9%97%AE%E9%A2%98"><span class="toc-text">4、 Rebalance 机制带来的问题</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85%E6%B6%88%E8%B4%B9%E5%88%86%E5%8C%BA%E7%9A%84%E4%B8%89%E7%A7%8D%E7%AD%96%E7%95%A5"><span class="toc-text">5、消费者消费分区的三种策略</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3%E3%80%81HW-%E4%B8%8E-LEO"><span class="toc-text">6.3、HW 与 LEO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-text">1、基本概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E5%AF%B9%E4%BA%8E%E6%96%B0%E5%86%99%E5%85%A5%E7%9A%84%E6%B6%88%E6%81%AF%E4%B8%8D%E8%83%BD%E7%AB%8B%E5%8D%B3%E6%B6%88%E8%B4%B9%EF%BC%9F"><span class="toc-text">2、为什么对于新写入的消息不能立即消费？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%83%E3%80%81Kafka-%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98"><span class="toc-text">七、Kafka 相关问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1%E3%80%81%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E6%B6%88%E6%81%AF%E4%B8%A2%E5%A4%B1"><span class="toc-text">7.1、如何防止消息丢失</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85"><span class="toc-text">1、生产者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85"><span class="toc-text">2、消费者</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2%E3%80%81%E5%A6%82%E4%BD%95%E9%98%B2%E6%AD%A2%E6%B6%88%E6%81%AF%E7%9A%84%E9%87%8D%E5%A4%8D%E6%B6%88%E8%B4%B9"><span class="toc-text">7.2、如何防止消息的重复消费</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3%E3%80%81%E5%A6%82%E4%BD%95%E5%81%9A%E5%88%B0%E9%A1%BA%E5%BA%8F%E6%B6%88%E8%B4%B9"><span class="toc-text">7.3、如何做到顺序消费</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85-1"><span class="toc-text">1、生产者</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%B6%88%E8%B4%B9%E8%80%85-1"><span class="toc-text">2、消费者</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4%E3%80%81%E5%A6%82%E4%BD%95%E8%A7%A3%E5%86%B3%E6%B6%88%E6%81%AF%E7%A7%AF%E5%8E%8B%E9%97%AE%E9%A2%98"><span class="toc-text">7.4、如何解决消息积压问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E9%97%AE%E9%A2%98%E8%AF%B4%E6%98%8E"><span class="toc-text">1、问题说明</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-text">2、解决方案</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-5%E3%80%81Kafka-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%BF%AB%EF%BC%9F"><span class="toc-text">7.5、Kafka 为什么快？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E7%A3%81%E7%9B%98%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99"><span class="toc-text">1、磁盘顺序读写</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E9%A1%B5%E7%BC%93%E5%AD%98"><span class="toc-text">2、页缓存</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E9%9B%B6%E6%8B%B7%E8%B4%9D"><span class="toc-text">3、零拷贝</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E6%89%B9%E5%A4%84%E7%90%86"><span class="toc-text">4、批处理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-6%E3%80%81Kafka-%E7%9A%84%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-text">7.6、Kafka 的幂等性</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81Kafka-%E7%94%9F%E4%BA%A7%E8%80%85%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-text">1、Kafka 生产者幂等性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E7%94%9F%E4%BA%A7%E8%80%85%E9%85%8D%E7%BD%AE%E5%B9%82%E7%AD%89%E6%80%A7"><span class="toc-text">2、生产者配置幂等性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81Kafka-%E4%BF%9D%E8%AF%81%E5%B9%82%E7%AD%89%E6%80%A7%E7%9A%84%E5%8E%9F%E7%90%86"><span class="toc-text">3、Kafka 保证幂等性的原理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-7%E3%80%81Kafka-%E7%9A%84%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8%E5%BD%A2%E5%BC%8F"><span class="toc-text">7.7、Kafka 的数据存储形式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81%E5%AD%98%E5%82%A8%E6%97%A5%E5%BF%97"><span class="toc-text">1、存储日志</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E5%86%99%E5%85%A5%E6%B6%88%E6%81%AF"><span class="toc-text">2、写入消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E8%AF%BB%E5%8F%96%E6%B6%88%E6%81%AF"><span class="toc-text">3、读取消息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%88%A0%E9%99%A4%E6%B6%88%E6%81%AF"><span class="toc-text">4、删除消息</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-8%E3%80%81Kafka-%E4%B8%AD%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86%EF%BC%88Log-Deletion%EF%BC%89"><span class="toc-text">7.8、Kafka 中数据清理（Log Deletion）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%E3%80%81Kafka-%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86%E9%85%8D%E7%BD%AE"><span class="toc-text">1、Kafka 数据清理配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2%E3%80%81%E6%97%A5%E5%BF%97%E5%88%A0%E9%99%A4"><span class="toc-text">2、日志删除</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3%E3%80%81%E5%9F%BA%E4%BA%8E%E6%97%B6%E9%97%B4%E7%9A%84%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5"><span class="toc-text">3、基于时间的保留策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4%E3%80%81%E5%9F%BA%E4%BA%8E%E6%97%A5%E5%BF%97%E5%A4%A7%E5%B0%8F%E7%9A%84%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5"><span class="toc-text">4、基于日志大小的保留策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5%E3%80%81%E5%9F%BA%E4%BA%8E%E6%97%A5%E5%BF%97%E8%B5%B7%E5%A7%8B%E5%81%8F%E7%A7%BB%E9%87%8F%E4%BF%9D%E7%95%99%E7%AD%96%E7%95%A5"><span class="toc-text">5、基于日志起始偏移量保留策略</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6%E3%80%81%E6%97%A5%E5%BF%97%E5%8E%8B%E7%BC%A9"><span class="toc-text">6、日志压缩</span></a></li></ol></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2022 <i style="color:#ff6a6a;animation:announ_animation .8s linear infinite" class="fa fa-heartbeat"></i> 天昕</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">欢迎来到我的个人博客!<span id="runtime"></span><br></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&amp;logo=hexo"></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&amp;logo=bitdefender"></a><a class="github-badge" target="_blank" href="https://www.jsdelivr.com/" style="margin-inline:5px"><img src="https://img.shields.io/badge/CDN-jsDelivr-orange?style=flat&amp;logo=jsDelivr"></a><a class="github-badge" target="_blank" href="https://gitee.com/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Picture-Gitee-0cedbe?style=flat&amp;logo=Gitee"></a><a class="github-badge" target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" style="margin-inline:5px"><img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&amp;logo=Claris"></a></p><div id="workboard"></div><script async src="/js/runtime.js"></script></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">簡</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"></div></div></div><hr><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><script>var preloader={endLoading:()=>{document.body.style.overflow="auto",document.getElementById("loading-box").classList.add("loaded")},initLoading:()=>{document.body.style.overflow="",document.getElementById("loading-box").classList.remove("loaded")}};window.addEventListener("load",preloader.endLoading())</script><script defer src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script defer src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script defer src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script defer src="/js/search/local-search.js"></script><div class="js-pjax"><script>(()=>{const t=document.getElementById("twikoo-count"),o=()=>{twikoo.init({el:"#twikoo-wrap",envId:"blogcomments-2gseqioe1aa55c8c",region:"ap-shanghai"})},e=()=>{twikoo.getCommentsCount({envId:"blogcomments-2gseqioe1aa55c8c",region:"ap-shanghai",urls:[window.location.pathname],includeReply:!1}).then(function(o){t.innerText=o[0].count}).catch(function(o){console.error(o)})};var n;n=!0,"object"==typeof twikoo?(o(),n&&t&&setTimeout(e,0)):getScript("https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js").then(()=>{o(),n&&t&&setTimeout(e,0)})})()</script></div><div class="aplayer no-destroy" data-id="6588965546" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listfolded="false" data-order="random" data-preload="none" data-autoplay="false" data-lrctype="0" muted></div><script defer src="/live2d-widget/autoload.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/flipcountdown.js"></script><script data-pjax src="/js/runtime.js"></script><script async src="//at.alicdn.com/t/font_2398185_yegv7kt2bj.js"></script><script src="https://apip.weatherdt.com/simple/static/js/weather-simple-common.js?v=2.0"></script><script src="/js/weather.js"></script><script src="/js/custom/runtime.js"></script><script src="https://cdn.jsdelivr.net/gh/weilain/cdn-photo/js/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/gh/zhheo/JS-Heo@main/hidescrollbar/hidescrollbar.js"></script><script async src="//at.alicdn.com/t/font_2398185_lld84dtfbb.js"></script><script src="https://www.luckyclover.top/rain.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful=!0,POWERMODE.shake=!1,POWERMODE.mobile=!1,document.body.addEventListener("input",POWERMODE)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload='this.media="all"'><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors=["title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"];var pjax=new Pjax({elements:'a:not([target="_blank"]):not([href="/shuoshuo/"]):not([href="/bb/"]):not([href="/contact/"])',selectors:pjaxSelectors,cacheBust:!1,analytics:!1,scrollRestoration:!1});document.addEventListener("pjax:complete",function(){window.refreshFn(),document.querySelectorAll("script[data-pjax], .pjax-reload script").forEach(e=>{const t=document.createElement("script"),o=e.text||e.textContent||e.innerHTML||"";Array.from(e.attributes).forEach(e=>t.setAttribute(e.name,e.value)),t.appendChild(document.createTextNode(o)),e.parentNode.replaceChild(t,e)}),GLOBAL_CONFIG.islazyload&&window.lazyLoadInstance.update(),"function"==typeof chatBtnFn&&chatBtnFn(),"function"==typeof panguInit&&panguInit(),"function"==typeof gtag&&gtag("config","",{page_path:window.location.pathname}),"function"==typeof loadMeting&&document.getElementsByClassName("aplayer").length&&loadMeting(),"object"==typeof Prism&&Prism.highlightAll(),"object"==typeof preloader&&preloader.endLoading()}),document.addEventListener("pjax:send",function(){if("object"==typeof preloader&&preloader.initLoading(),window.aplayers)for(let e=0;e<window.aplayers.length;e++)window.aplayers[e].options.fixed||window.aplayers[e].destroy();"object"==typeof typed&&typed.destroy();const e=document.body.classList;e.contains("read-mode")&&e.remove("read-mode")}),document.addEventListener("pjax:error",e=>{404===e.request.status&&pjax.loadUrl("/404.html")})</script></div><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.10/hexo_githubcalendar.js"></script><script data-pjax>function GithubCalendarConfig(){var e=document.getElementById("recent-posts");e&&e.insertAdjacentHTML("afterbegin",'<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_container"></div></div>'),GithubCalendar("https://python-github-calendar-api.vercel.app/api?sutianxin",["#ebedf0","#f1f8ff","#dbedff","#c8e1ff","#79b8ff","#2188ff","#0366d6","#005cc5","#044289","#032f62","#05264c"],"sutianxin")}document.getElementById("recent-posts")&&GithubCalendarConfig()</script><style>#github_container{min-height:280px}@media screen and (max-width:650px){#github_container{min-height:120px}}</style></body></html>